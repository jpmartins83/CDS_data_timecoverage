{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aerosols\n",
      "['satellite-aerosol-properties']\n",
      "/Users/cxjo/Documents/cds-forms-c3s/satellite-aerosol-properties/constraints.json\n",
      "Beware error in allowed dates...\n",
      "Beware error in allowed dates...\n",
      "Albedo\n",
      "['satellite-albedo']\n",
      "/Users/cxjo/Documents/cds-forms-c3s/satellite-albedo/constraints.json\n",
      "Greenhouse Gases\n",
      "['satellite-carbon-dioxide', 'satellite-methane']\n",
      "/Users/cxjo/Documents/cds-forms-c3s/satellite-carbon-dioxide/constraints.json\n",
      "/Users/cxjo/Documents/cds-forms-c3s/satellite-methane/constraints.json\n",
      "Clouds\n",
      "['satellite-cloud-properties']\n",
      "/Users/cxjo/Documents/cds-forms-c3s/satellite-cloud-properties/constraints.json\n",
      "Earth Radiation Budget\n",
      "['satellite-earth-radiation-budget']\n",
      "/Users/cxjo/Documents/cds-forms-c3s/satellite-earth-radiation-budget/constraints.json\n",
      "Fire\n",
      "['satellite-fire-burned-area', 'satellite-fire-radiative-power']\n",
      "/Users/cxjo/Documents/cds-forms-c3s/satellite-fire-burned-area/constraints.json\n",
      "/Users/cxjo/Documents/cds-forms-c3s/satellite-fire-radiative-power/constraints.json\n",
      "Ice Sheets\n",
      "['satellite-greenland-ice-sheet-velocity', 'satellite-ice-sheet-elevation-change', 'satellite-ice-sheet-mass-balance', 'derived-gridded-glacier-mass-change/']\n",
      "/Users/cxjo/Documents/cds-forms-c3s/satellite-greenland-ice-sheet-velocity/constraints.json\n",
      "/Users/cxjo/Documents/cds-forms-c3s/satellite-ice-sheet-elevation-change/constraints.json\n",
      "/Users/cxjo/Documents/cds-forms-c3s/satellite-ice-sheet-mass-balance/constraints.json\n",
      "/Users/cxjo/Documents/cds-forms-c3s/derived-gridded-glacier-mass-change//constraints.json\n",
      "Glaciers\n",
      "['insitu-glaciers-extent']\n",
      "/Users/cxjo/Documents/cds-forms-c3s/insitu-glaciers-extent/constraints.json\n",
      "Upper-air Water Vapour\n",
      "['satellite-humidity-profiles', 'satellite-total-column-water-vapour-land-ocean', 'satellite-total-column-water-vapour-ocean', 'satellite-upper-troposphere-humidity']\n",
      "/Users/cxjo/Documents/cds-forms-c3s/satellite-humidity-profiles/constraints.json\n",
      "/Users/cxjo/Documents/cds-forms-c3s/satellite-total-column-water-vapour-land-ocean/constraints.json\n",
      "/Users/cxjo/Documents/cds-forms-c3s/satellite-total-column-water-vapour-ocean/constraints.json\n",
      "/Users/cxjo/Documents/cds-forms-c3s/satellite-upper-troposphere-humidity/constraints.json\n",
      "LAI\n",
      "['satellite-lai-fapar']\n",
      "/Users/cxjo/Documents/cds-forms-c3s/satellite-lai-fapar/constraints.json\n",
      "FAPAR\n",
      "['satellite-lai-fapar']\n",
      "/Users/cxjo/Documents/cds-forms-c3s/satellite-lai-fapar/constraints.json\n",
      "Lakes\n",
      "['satellite-lake-water-level', 'satellite-lake-water-temperature']\n",
      "/Users/cxjo/Documents/cds-forms-c3s/satellite-lake-water-level/constraints.json\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Timestamp' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[258], line 270\u001b[0m\n\u001b[1;32m    268\u001b[0m     datemax_list\u001b[38;5;241m.\u001b[39mappend(datemax)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m jfilepath \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/cxjo/Documents/cds-forms-c3s/satellite-lake-water-level/constraints.json\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 270\u001b[0m     datemin,datemax \u001b[38;5;241m=\u001b[39m extract_dates_lake_levels()\n\u001b[1;32m    271\u001b[0m     datemin_list\u001b[38;5;241m.\u001b[39mappend(datemin)\n\u001b[1;32m    272\u001b[0m     datemax_list\u001b[38;5;241m.\u001b[39mappend(datemax)\n",
      "Cell \u001b[0;32mIn[258], line 44\u001b[0m, in \u001b[0;36mextract_dates_lake_levels\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/cxjo/Downloads/C3S_LWL_N-AFRICA_VOLTA_altimetry_4.0_19921013_20221222_R20230126.nc\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     43\u001b[0m nc \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(fname)\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mTimestamp(nc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime64[D]\u001b[39m\u001b[38;5;124m'\u001b[39m),pd\u001b[38;5;241m.\u001b[39mTimestamp(nc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime64[D]\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Timestamp' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import os\n",
    "from datetime import datetime,date\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import json\n",
    "import yaml\n",
    "from calendar import monthrange\n",
    "import xarray as xr\n",
    "\n",
    "#important\n",
    "#before running this script, please git pull, then git checkout c3stest\n",
    "\n",
    "\n",
    "def extract_dates_from_TSI():\n",
    "    url='https://gerb.oma.be/tsi/C3S_RMIB_daily_TSI_composite_ICDR_v3_latest.txt'\n",
    "    c=pd.read_csv(url,skiprows=128,delim_whitespace=True,header=None)\n",
    "    return pd.Timestamp(str(c[3].iloc[0])),pd.Timestamp(str(c[3].iloc[-1]))\n",
    "def extract_dates_icesheets():\n",
    "    fname = '/Users/cxjo/Downloads/C3S_GrIS_RA_SEC_25km_vers4_2024-01-15.nc'\n",
    "    nc = xr.open_dataset(fname)\n",
    "    return pd.Timestamp(nc['time'].values[0]),pd.Timestamp(nc['time'].values[-1])\n",
    "def extract_dates_massbalance():\n",
    "    fname = '/Users/cxjo/Downloads/C3S_GMB_GRACE_vers4.nc'\n",
    "    nc = xr.open_dataset(fname)\n",
    "    return pd.Timestamp(nc['time'].values[0]),pd.Timestamp(nc['time'].values[-1])\n",
    "def extract_dates_derived_glaciers(jfile):\n",
    "    with open(jfile) as f:\n",
    "        gen= yaml.safe_load(f)\n",
    "    years = gen['labels']['hydrological_year'].keys()\n",
    "    ymin=int(min(years))\n",
    "    ymax=int(max(years))+1\n",
    "    mmax=9\n",
    "    dmax=30\n",
    "    mmin=4\n",
    "    dmin=1\n",
    "    return pd.Timestamp(f'{ymin}-{mmin}-{dmin}'), pd.Timestamp(f'{ymax}-{mmax}-{dmax}')\n",
    "def extract_dates_lake_levels():\n",
    "    # need better way of estimating this\n",
    "    fname = '/Users/cxjo/Downloads/C3S_LWL_N-AFRICA_VOLTA_altimetry_4.0_19921013_20221222_R20230126.nc'\n",
    "    nc = xr.open_dataset(fname)\n",
    "    return pd.Timestamp(nc['time'].values[0],unit='D'),pd.Timestamp(nc['time'].values[-1],unit='D')\n",
    "def datemax2(row):\n",
    "    row = row.dropna()\n",
    "    ymax = int(max(row['year']))\n",
    "    if 'month' in row.keys():\n",
    "        mmax = int(max(row['month']))\n",
    "    else:\n",
    "        mmax=12\n",
    "    xx,dmax=monthrange(ymax,mmax)\n",
    "    datemax = pd.Timestamp(f'{ymax}-{mmax}-{dmax}')\n",
    "    return datemax\n",
    "def datemin2(row):\n",
    "    row = row.dropna()\n",
    "    ymin = int(min(row['year']))\n",
    "    if 'month' in row.keys():\n",
    "        mmin = int(min(row['month']))\n",
    "    else:\n",
    "        mmin=1\n",
    "    xx,dmin=monthrange(ymin,mmin)\n",
    "    datemin = pd.Timestamp(f'{ymin}-{mmin}-{dmin}')\n",
    "    return datemin\n",
    "def extract_dates_wv(jfilepath):\n",
    "    f = open(jfilepath)\n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    data = json.load(f)\n",
    "    # print(data)\n",
    "    df = pd.DataFrame(data)\n",
    "    df['datemax'] = df.apply(datemax2,axis=1)\n",
    "    df['datemin'] = df.apply(datemin2,axis=1)\n",
    "    datemin = df['datemin'].min()\n",
    "    datemax = df['datemax'].max()\n",
    "    return datemin,datemax \n",
    "def check_time_agg(row):\n",
    "    row = row.dropna()\n",
    "    if 'time_aggregation' in row.keys():\n",
    "        time_aggregation = row['time_aggregation'][0]\n",
    "        # print('time_agg',time_aggregation)\n",
    "        if time_aggregation in ['daily_average','daily_mean','day','day_average']: \n",
    "            time_agg = 'day'\n",
    "            return time_agg\n",
    "        elif time_aggregation == 'daily':\n",
    "            time_agg='daily'\n",
    "            return time_agg\n",
    "        #note interim solution for 5-daily-composite...\n",
    "        elif time_aggregation in [\n",
    "            'monthly_average',\n",
    "            '5_daily_composite',\n",
    "            'monthly_mean',\n",
    "            '27_days',\n",
    "            'month',\n",
    "            'monthly',\n",
    "            'month_average',\n",
    "            '10_day_average', # debatable..\n",
    "            ]:\n",
    "            time_agg = 'monthly'\n",
    "            return time_agg\n",
    "        else:\n",
    "            # print(row)\n",
    "            print('Could not determine time_agg')\n",
    "            raise SystemExit\n",
    "    elif 'period' in row.keys(): # applies to ice_sheets\n",
    "        time_agg = 'period'\n",
    "        return time_agg\n",
    "    elif 'temporal_aggregation' in row.keys():\n",
    "        time_aggregation = row['temporal_aggregation'][0]\n",
    "        if time_aggregation in ['monthly','6-hourly']:\n",
    "            time_agg='monthly'\n",
    "        elif time_aggregation == 'daily':\n",
    "            time_agg='daily'\n",
    "        else:\n",
    "            print('Error in temporal aggregation')\n",
    "            raise SystemExit\n",
    "        return time_agg\n",
    "    else:\n",
    "        if ('day' in row.keys()):\n",
    "            time_agg='day'\n",
    "            return time_agg\n",
    "        elif  'nominal_day' in row.keys():\n",
    "            time_agg='nominal_day'\n",
    "            return time_agg\n",
    "        else:\n",
    "            time_agg = 'monthly'\n",
    "            return time_agg\n",
    "def compute_datemax(row):\n",
    " \n",
    "    time_agg=check_time_agg(row) # check time aggregation of data in this row\n",
    "    # print('time_agg',time_agg)\n",
    "    if time_agg =='period':\n",
    "        per_str=max(row['period'])\n",
    "        ymax=int(per_str[5::])\n",
    "        mmax=9\n",
    "        dmax=30\n",
    "    else: \n",
    "        ymax = int(max(row['year']))\n",
    "        if 'month' in row.keys():\n",
    "            mmax = int(max(row['month']))\n",
    "        else:\n",
    "            mmax=12\n",
    "        xx,ndays = monthrange(ymax,mmax)\n",
    "        if time_agg in ['day','nominal_day']:\n",
    "            # print(row[time_agg])\n",
    "            dmax = int(max(row[time_agg])) \n",
    "        elif time_agg in ['daily']:\n",
    "            dmax = int(max(row['day'])) \n",
    "        else:\n",
    "            dmax = ndays # last day of month\n",
    "        if dmax >ndays:\n",
    "            print('Beware error in allowed dates...')\n",
    "            # print(row)\n",
    "            dmax=ndays\n",
    "\n",
    "    datemax = pd.Timestamp(f'{ymax}-{mmax}-{dmax}')\n",
    "    return datemax\n",
    "def compute_datemin(row):\n",
    "    time_agg=check_time_agg(row) # check time aggregation of data in this row\n",
    "    if time_agg =='period':\n",
    "        per_str=max(row['period'])\n",
    "        ymin=int(per_str[0:4])\n",
    "        mmin=10\n",
    "        dmin=1\n",
    "    else:\n",
    "        ymin = int(min(row['year'])) \n",
    "        if 'month' in row.keys():\n",
    "            mmin = int(min(row['month']))\n",
    "        else:\n",
    "            mmin=1\n",
    "        if time_agg in ['day','nominal_day']:\n",
    "            dmin = int(min(row[time_agg])) \n",
    "        elif time_agg in ['daily']:\n",
    "            dmin = int(min(row['day'])) \n",
    "        else:\n",
    "            dmin = 1 # first day of month\n",
    "        xx,ndays = monthrange(ymin,mmin)     \n",
    "        if dmin>ndays:\n",
    "            print('Beware error in allowed dates...')\n",
    "            # print(row)\n",
    "            dmin=1\n",
    "    datemin = pd.Timestamp(f'{ymin}-{mmin}-{dmin}')\n",
    "    return datemin\n",
    "def calc_dateminmax_from_cds_form(jfilepath,ecv):\n",
    "    # Opening JSON file\n",
    "    f = open(jfilepath)\n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    data = json.load(f)\n",
    "    # print(data)\n",
    "    df = pd.DataFrame(data)\n",
    "    # display(df)\n",
    "    # print(df.keys())\n",
    "    # print(len(df))\n",
    "    \n",
    "    # find records where dates cannot be defined\n",
    "    if 'sensor_and_algorithm' in df.keys():        \n",
    "        lst_erase=[]\n",
    "        for i in range(len(df)):\n",
    "            if (df['sensor_and_algorithm'][i][0]=='merged_obs4mips'): lst_erase.append(i)\n",
    "        # now .drop these problematic rows\n",
    "        for i in lst_erase:\n",
    "            df=df.drop(lst_erase)\n",
    "    if ecv == 'Earth Radiation Budget':        \n",
    "        lst_erase=[]\n",
    "        for i in range(len(df)):\n",
    "            if (df['variable'][i][0]=='total_solar_irradiance'): lst_erase.append(i) # this info is read from the dataset itself\n",
    "        # now .drop these problematic rows\n",
    "        for i in lst_erase:\n",
    "            df=df.drop(lst_erase)\n",
    "    # for i in range(len(df)):\n",
    "    #     print(df.loc[i])\n",
    "    #     if ('year' not in df[i]): lst_erase.append(i)\n",
    "\n",
    "    df['datemax'] = df.apply(compute_datemax,axis=1)\n",
    "    df['datemin'] = df.apply(compute_datemin,axis=1)\n",
    "    datemin = df['datemin'].min()\n",
    "    datemax = df['datemax'].max()\n",
    "    return datemin,datemax\n",
    "\n",
    "with open('config.yml') as f:\n",
    "    conf= yaml.safe_load(f)\n",
    "\n",
    "cds_form_dir=conf['cds_form_dir']\n",
    "\n",
    "\n",
    "datesbeg = {}\n",
    "datesend = {}\n",
    "\n",
    "for ecv in conf['ECV']:\n",
    "    print(ecv)\n",
    "    entries = conf['ECV'][ecv]['entry']\n",
    "    print(entries)\n",
    "    datemin_list = []\n",
    "    datemax_list = []\n",
    "    if ecv in ['Earth Radiation Budget']: \n",
    "        datemin,datemax = extract_dates_from_TSI()\n",
    "        datemin_list.append(datemin)\n",
    "        datemax_list.append(datemax)\n",
    "    for entry in entries:\n",
    "        jfilepath=f'{cds_form_dir}{entry}/constraints.json'\n",
    "        print(jfilepath)\n",
    "        if jfilepath == '/Users/cxjo/Documents/cds-forms-c3s/satellite-ice-sheet-elevation-change/constraints.json':\n",
    "            datemin,datemax = extract_dates_icesheets()\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        elif jfilepath == '/Users/cxjo/Documents/cds-forms-c3s/satellite-ice-sheet-mass-balance/constraints.json':\n",
    "            datemin,datemax = extract_dates_massbalance()\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        elif jfilepath == '/Users/cxjo/Documents/cds-forms-c3s/derived-gridded-glacier-mass-change//constraints.json':\n",
    "            jfile = f'{cds_form_dir}{entry}/generate.yaml'\n",
    "            datemin,datemax = extract_dates_derived_glaciers(jfile)\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        elif jfilepath == '/Users/cxjo/Documents/cds-forms-c3s/insitu-glaciers-extent/constraints.json':    \n",
    "            datemin= pd.Timestamp('1990-01-01') # http://www.glims.org/rgi_user_guide/06_dataset_summary.html\n",
    "            datemax= pd.Timestamp('2010-12-31')\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        elif jfilepath =='/Users/cxjo/Documents/cds-forms-c3s/satellite-total-column-water-vapour-ocean/constraints.json':\n",
    "            # temporal aggregation is messed up. does not have the same meaning as other datasets\n",
    "            # monthly should be yearly\n",
    "            # 6-hourly should be monthly\n",
    "            # need to write a special function that accounts for this\n",
    "            datemin,datemax = extract_dates_wv(jfilepath)\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        elif jfilepath == '/Users/cxjo/Documents/cds-forms-c3s/satellite-lake-water-level/constraints.json':\n",
    "            datemin,datemax = extract_dates_lake_levels()\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        else:\n",
    "            datemin,datemax = calc_dateminmax_from_cds_form(jfilepath,ecv)\n",
    "            # print(ecv,datemin_list,datemax_list)\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "            # print(ecv,datemin_list,datemax_list)\n",
    "\n",
    "    # now get the max and min per ECV, accounting for all products\n",
    "    datemin_list = np.array(datemin_list)\n",
    "    datemax_list = np.array(datemax_list)\n",
    "    datesbeg[ecv] = np.min(datemin_list)\n",
    "    datesend[ecv] = np.max(datemax_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               DateBeg  \\\n",
      "Aerosols                 1995-06-01 00:00:00.000000000   \n",
      "Albedo                   1981-09-20 00:00:00.000000000   \n",
      "Greenhouse Gases         2002-10-01 00:00:00.000000000   \n",
      "Clouds                   1982-01-01 00:00:00.000000000   \n",
      "Earth Radiation Budget   1979-01-01 00:00:00.000000000   \n",
      "Fire                     2001-01-01 00:00:00.000000000   \n",
      "Ice Sheets               1975-04-01 00:00:00.000000000   \n",
      "Glaciers                 1990-01-01 00:00:00.000000000   \n",
      "Upper-air Water Vapour   1988-01-31 00:00:00.000000000   \n",
      "LAI                      1981-09-20 00:00:00.000000000   \n",
      "FAPAR                    1981-09-20 00:00:00.000000000   \n",
      "Lakes                    1992-10-13 13:52:00.002174464   \n",
      "Land Cover               1992-01-01 00:00:00.000000000   \n",
      "Ocean Colour             1997-09-04 00:00:00.000000000   \n",
      "Ozone                    1970-04-01 00:00:00.000000000   \n",
      "Precipitation            1979-01-01 00:00:00.000000000   \n",
      "Sea Ice                  1978-10-25 00:00:00.000000000   \n",
      "Sea Level                1993-01-01 00:00:00.000000000   \n",
      "SST                      1979-07-11 00:00:00.000000000   \n",
      "Soil Moisture            1978-11-01 00:00:00.000000000   \n",
      "Surface Currents         1993-01-01 00:00:00.000000000   \n",
      "Surface Radiation Budget 1982-01-01 00:00:00.000000000   \n",
      "\n",
      "                                               DateEnd  \n",
      "Aerosols                 2022-12-31 00:00:00.000000000  \n",
      "Albedo                   2020-06-30 00:00:00.000000000  \n",
      "Greenhouse Gases         2021-12-31 00:00:00.000000000  \n",
      "Clouds                   2022-06-30 00:00:00.000000000  \n",
      "Earth Radiation Budget   2024-02-08 00:00:00.000000000  \n",
      "Fire                     2022-04-01 00:00:00.000000000  \n",
      "Ice Sheets               2023-07-01 00:00:00.000000000  \n",
      "Glaciers                 2010-12-31 00:00:00.000000000  \n",
      "Upper-air Water Vapour   2021-08-31 00:00:00.000000000  \n",
      "LAI                      2020-06-30 00:00:00.000000000  \n",
      "FAPAR                    2020-06-30 00:00:00.000000000  \n",
      "Lakes                    2022-12-22 22:52:59.997825536  \n",
      "Land Cover               2020-12-31 00:00:00.000000000  \n",
      "Ocean Colour             2023-03-31 00:00:00.000000000  \n",
      "Ozone                    2023-04-30 00:00:00.000000000  \n",
      "Precipitation            2021-06-30 00:00:00.000000000  \n",
      "Sea Ice                  2023-11-08 00:00:00.000000000  \n",
      "Sea Level                2022-08-04 00:00:00.000000000  \n",
      "SST                      2022-12-31 00:00:00.000000000  \n",
      "Soil Moisture            2023-10-31 00:00:00.000000000  \n",
      "Surface Currents         2022-08-04 00:00:00.000000000  \n",
      "Surface Radiation Budget 2022-06-30 00:00:00.000000000  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "ecv_pd = pd.DataFrame([datesbeg,datesend],index=['DateBeg','DateEnd']).T\n",
    "ecv_pd['DateBeg'] = ecv_pd['DateBeg'].astype('datetime64[D]')\n",
    "print(ecv_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datemin: 1992-01-01 00:00:00 datemax: 2023-07-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
