{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime,date\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import os\n",
    "from datetime import datetime,date\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import json\n",
    "import yaml\n",
    "from calendar import monthrange\n",
    "import xarray as xr\n",
    "import cdsapi\n",
    "import zipfile\n",
    "from glob import glob\n",
    "#important\n",
    "#before running this script, please git pull, then git checkout c3sprod\n",
    "\n",
    "\n",
    "# wishlist\n",
    "# - put land hydrology just below cryosphere\n",
    "# - update repository and change to c3sprod\n",
    "#   - git checkout c3sprod\n",
    "#   - git pull\n",
    "# - download datasets that only contain info in the time dimension from CDS\n",
    "# - try to make the code more path-independent\n",
    "# - install on athos and run every week. update some figure repo\n",
    "\n",
    "\n",
    "\n",
    "def extract_dates_from_TSI():\n",
    "    url='https://gerb.oma.be/tsi/C3S_RMIB_daily_TSI_composite_ICDR_v3_latest.txt'\n",
    "    c=pd.read_csv(url,skiprows=128,delim_whitespace=True,header=None)\n",
    "    return pd.Timestamp(str(c[3].iloc[0])),pd.Timestamp(str(c[3].iloc[-1]))\n",
    "def extract_dates_icesheets(datasets_dir):\n",
    "    # opens TCDR and ICDR files for both Antarctica and Greenland, then computes max/min dates\n",
    "    # needs adjustment for per product sorting\n",
    "    fnames = glob(datasets_dir+'C3S_*IS_RA*.nc')\n",
    "    datebegs = []\n",
    "    dateends = []\n",
    "    for fname in fnames:\n",
    "        nc = xr.open_dataset(fname)\n",
    "        datebegs.append(pd.Timestamp(nc['time'].values[0]))\n",
    "        dateends.append(pd.Timestamp(nc['time'].values[-1]))\n",
    "    return min(datebegs),max(dateends)\n",
    "def extract_dates_massbalance(datasets_dir):\n",
    "    fname = datasets_dir+'C3S_GMB_GRACE_vers4.nc'\n",
    "    nc = xr.open_dataset(fname)\n",
    "    return pd.Timestamp(nc['time'].values[0]),pd.Timestamp(nc['time'].values[-1])\n",
    "def extract_dates_derived_glaciers(jfile):\n",
    "    with open(jfile) as f:\n",
    "        gen= yaml.safe_load(f)\n",
    "    years = gen['labels']['hydrological_year'].keys()\n",
    "    ymin=int(min(years))\n",
    "    ymax=int(max(years))+1\n",
    "    mmax=9\n",
    "    dmax=30\n",
    "    mmin=4\n",
    "    dmin=1\n",
    "    print('Glaciers ',ymin,ymax)\n",
    "    return pd.Timestamp(f'{ymin}-{mmin}-{dmin}'), pd.Timestamp(f'{ymax}-{mmax}-{dmax}')\n",
    "def extract_dates_lake_levels(datasets_dir):\n",
    "    # need better way of estimating this\n",
    "    # for v4, according to overview pages, coverage goes until 2022\n",
    "    # need to implement download of all lakes data and check their time coverage\n",
    "    print('...for lake water levels, only reading info for Lake Volta...')\n",
    "    fname = datasets_dir+'C3S_LWL_N-AFRICA_VOLTA_altimetry_4.0_19921013_20221222_R20230126.nc'\n",
    "    nc = xr.open_dataset(fname)\n",
    "    return pd.Timestamp(nc['time'].values[0]),pd.Timestamp(nc['time'].values[-1])\n",
    "def datemax2(row):\n",
    "    row = row.dropna()\n",
    "    ymax = int(max(row['year']))\n",
    "    if 'month' in row.keys():\n",
    "        mmax = int(max(row['month']))\n",
    "    else:\n",
    "        mmax=12\n",
    "    xx,dmax=monthrange(ymax,mmax)\n",
    "    datemax = pd.Timestamp(f'{ymax}-{mmax}-{dmax}')\n",
    "    return datemax\n",
    "def datemin2(row):\n",
    "    row = row.dropna()\n",
    "    ymin = int(min(row['year']))\n",
    "    if 'month' in row.keys():\n",
    "        mmin = int(min(row['month']))\n",
    "    else:\n",
    "        mmin=1\n",
    "    xx,dmin=monthrange(ymin,mmin)\n",
    "    datemin = pd.Timestamp(f'{ymin}-{mmin}-{dmin}')\n",
    "    return datemin\n",
    "def extract_dates_wv(jfilepath):\n",
    "    f = open(jfilepath)\n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    data = json.load(f)\n",
    "    # print(data)\n",
    "    df = pd.DataFrame(data)\n",
    "    df['datemax'] = df.apply(datemax2,axis=1)\n",
    "    df['datemin'] = df.apply(datemin2,axis=1)\n",
    "    datemin = df['datemin'].min()\n",
    "    datemax = df['datemax'].max()\n",
    "    return datemin,datemax \n",
    "def check_time_agg(row):\n",
    "    row = row.dropna()\n",
    "    if 'time_aggregation' in row.keys():\n",
    "        time_aggregation = row['time_aggregation'][0]\n",
    "        # print('time_agg',time_aggregation)\n",
    "        if time_aggregation in ['daily_average','daily_mean','day','day_average']: \n",
    "            time_agg = 'day'\n",
    "            return time_agg\n",
    "        elif time_aggregation == 'daily':\n",
    "            time_agg='daily'\n",
    "            return time_agg\n",
    "        #note interim solution for 5-daily-composite...\n",
    "        elif time_aggregation in [\n",
    "            'monthly_average',\n",
    "            '5_daily_composite',\n",
    "            'monthly_mean',\n",
    "            '27_days',\n",
    "            'month',\n",
    "            'monthly',\n",
    "            'month_average',\n",
    "            '10_day_average', # debatable..\n",
    "            ]:\n",
    "            time_agg = 'monthly'\n",
    "            return time_agg\n",
    "        else:\n",
    "            # print(row)\n",
    "            print('Could not determine time_agg')\n",
    "            raise SystemExit\n",
    "    elif 'period' in row.keys(): # applies to ice_sheets\n",
    "        time_agg = 'period'\n",
    "        return time_agg\n",
    "    elif 'temporal_aggregation' in row.keys():\n",
    "        time_aggregation = row['temporal_aggregation'][0]\n",
    "        if time_aggregation in ['monthly','6-hourly']:\n",
    "            time_agg='monthly'\n",
    "        elif time_aggregation == 'daily':\n",
    "            time_agg='daily'\n",
    "        else:\n",
    "            print('Error in temporal aggregation')\n",
    "            raise SystemExit\n",
    "        return time_agg\n",
    "    else:\n",
    "        if ('day' in row.keys()):\n",
    "            time_agg='day'\n",
    "            return time_agg\n",
    "        elif  'nominal_day' in row.keys():\n",
    "            time_agg='nominal_day'\n",
    "            return time_agg\n",
    "        else:\n",
    "            time_agg = 'monthly'\n",
    "            return time_agg\n",
    "def compute_datemax(row):\n",
    " \n",
    "    time_agg=check_time_agg(row) # check time aggregation of data in this row\n",
    "    # print('time_agg',time_agg)\n",
    "    if time_agg =='period':\n",
    "        per_str=max(row['period'])\n",
    "        ymax=int(per_str[5::])\n",
    "        mmax=9\n",
    "        dmax=30\n",
    "    else: \n",
    "        ymax = int(max(row['year']))\n",
    "        if 'month' in row.keys():\n",
    "            mmax = int(max(row['month']))\n",
    "        else:\n",
    "            mmax=12\n",
    "        xx,ndays = monthrange(ymax,mmax)\n",
    "        if time_agg in ['day','nominal_day']:\n",
    "            # print(row[time_agg])\n",
    "            dmax = int(max(row[time_agg])) \n",
    "        elif time_agg in ['daily']:\n",
    "            dmax = int(max(row['day'])) \n",
    "        else:\n",
    "            dmax = ndays # last day of month\n",
    "        if dmax >ndays:\n",
    "            print('Beware error in allowed dates...')\n",
    "            # print(row)\n",
    "            dmax=ndays\n",
    "\n",
    "    datemax = pd.Timestamp(f'{ymax}-{mmax}-{dmax}')\n",
    "    return datemax\n",
    "def compute_datemin(row):\n",
    "    time_agg=check_time_agg(row) # check time aggregation of data in this row\n",
    "    if time_agg =='period':\n",
    "        per_str=max(row['period'])\n",
    "        ymin=int(per_str[0:4])\n",
    "        mmin=10\n",
    "        dmin=1\n",
    "    else:\n",
    "        ymin = int(min(row['year'])) \n",
    "        if 'month' in row.keys():\n",
    "            mmin = int(min(row['month']))\n",
    "        else:\n",
    "            mmin=1\n",
    "        if time_agg in ['day','nominal_day']:\n",
    "            dmin = int(min(row[time_agg])) \n",
    "        elif time_agg in ['daily']:\n",
    "            dmin = int(min(row['day'])) \n",
    "        else:\n",
    "            dmin = 1 # first day of month\n",
    "        xx,ndays = monthrange(ymin,mmin)     \n",
    "        if dmin>ndays:\n",
    "            print('Beware error in allowed dates...')\n",
    "            # print(row)\n",
    "            dmin=1\n",
    "    datemin = pd.Timestamp(f'{ymin}-{mmin}-{dmin}')\n",
    "    return datemin\n",
    "def calc_dateminmax_from_cds_form(jfilepath,ecv):\n",
    "    # Opening JSON file\n",
    "    f = open(jfilepath)\n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    data = json.load(f)\n",
    "    # print(data)\n",
    "    df = pd.DataFrame(data)\n",
    "    # display(df)\n",
    "    # print(df.keys())\n",
    "    # print(len(df))\n",
    "    \n",
    "    # find records where dates cannot be defined\n",
    "    if 'sensor_and_algorithm' in df.keys():        \n",
    "        lst_erase=[]\n",
    "        for i in range(len(df)):\n",
    "            if (df['sensor_and_algorithm'][i][0]=='merged_obs4mips'): lst_erase.append(i)\n",
    "        # now .drop these problematic rows\n",
    "        for i in lst_erase:\n",
    "            df=df.drop(lst_erase)\n",
    "    if ecv == 'Earth Radiation Budget':        \n",
    "        lst_erase=[]\n",
    "        for i in range(len(df)):\n",
    "            if (df['variable'][i][0]=='total_solar_irradiance'): lst_erase.append(i) # this info is read from the dataset itself\n",
    "        # now .drop these problematic rows\n",
    "        for i in lst_erase:\n",
    "            df=df.drop(lst_erase)\n",
    "    # for i in range(len(df)):\n",
    "    #     print(df.loc[i])\n",
    "    #     if ('year' not in df[i]): lst_erase.append(i)\n",
    "\n",
    "    df['datemax'] = df.apply(compute_datemax,axis=1)\n",
    "    df['datemin'] = df.apply(compute_datemin,axis=1)\n",
    "    datemin = df['datemin'].min()\n",
    "    datemax = df['datemax'].max()\n",
    "    return datemin,datemax\n",
    "def datasets_download(datasets_dir):\n",
    "    c = cdsapi.Client()\n",
    "    # c.retrieve(\n",
    "    #     'satellite-ice-sheet-mass-balance',\n",
    "    #     {\n",
    "    #         'variable': 'all',\n",
    "    #         'format': 'zip',\n",
    "    #     },\n",
    "    #     datasets_dir+'download.zip')\n",
    "    # os.system(f'cd {datasets_dir} ; unzip download.zip; rm -rf download.zip ; cd ..')\n",
    "    \n",
    "    # Antarctica Ice sheet surface elevation change TCDR\n",
    "    # c.retrieve(\n",
    "    # 'satellite-ice-sheet-elevation-change',\n",
    "    # {\n",
    "    #     'domain': 'antarctica',\n",
    "    #     'climate_data_record_type': 'tcdr',\n",
    "    #     'version': '4_0',\n",
    "    #     'variable': 'all',\n",
    "    #     'format': 'zip',\n",
    "    # },\n",
    "    # datasets_dir+'download.zip')\n",
    "    # os.system(f'cd {datasets_dir} ; unzip download.zip; rm -rf download.zip ; cd ..')\n",
    "    \n",
    "    # Antarctica Ice sheet surface elevation change ICDR\n",
    "    # c.retrieve(\n",
    "    #     'satellite-ice-sheet-elevation-change',\n",
    "    # {\n",
    "    #     'domain': 'antarctica',\n",
    "    #     'climate_data_record_type': 'icdr',\n",
    "    #     'version': '3_0',\n",
    "    #     'variable': 'all',\n",
    "    #     'format': 'zip',\n",
    "    # },\n",
    "    # datasets_dir+'download.zip')\n",
    "    # os.system(f'cd {datasets_dir} ; unzip download.zip; rm -rf download.zip ; cd ..')\n",
    "    \n",
    "    \n",
    "    # Greenland Ice sheet surface elevation change TCDR\n",
    "    # c.retrieve(\n",
    "    #     'satellite-ice-sheet-elevation-change',\n",
    "    # {\n",
    "    #     'domain': 'greenland',\n",
    "    #     'climate_data_record_type': 'tcdr',\n",
    "    #     'version': '4_0',\n",
    "    #     'variable': 'all',\n",
    "    #     'format': 'zip',\n",
    "    # },\n",
    "    # datasets_dir+'download.zip')\n",
    "    # os.system(f'cd {datasets_dir} ; unzip download.zip; rm -rf download.zip ; cd ..')\n",
    "    \n",
    "    # Greenland Ice sheet surface elevation change ICDR\n",
    "    # c.retrieve(\n",
    "    #     'satellite-ice-sheet-elevation-change',\n",
    "    # {\n",
    "    #     'domain': 'greenland',\n",
    "    #     'climate_data_record_type': 'icdr',\n",
    "    #     'version': '4_0',\n",
    "    #     'variable': 'all',\n",
    "    #     'format': 'zip',\n",
    "    # },\n",
    "    # datasets_dir+'download.zip')\n",
    "    # os.system(f'cd {datasets_dir} ; unzip download.zip; rm -rf download.zip ; cd ..')\n",
    "    \n",
    "    # Lake water levels\n",
    "    \n",
    "    c.retrieve(\n",
    "        'satellite-lake-water-level',\n",
    "    {\n",
    "        'variable': 'all',\n",
    "        'version': 'version_4_0',\n",
    "        'format': 'zip',\n",
    "        'region': 'northern_africa',\n",
    "        'lake': 'volta',\n",
    "    },\n",
    "    datasets_dir+'download.zip')\n",
    "    os.system(f'cd {datasets_dir} ; unzip download.zip; rm -rf download.zip ; cd ..')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return\n",
    "with open('config.yml') as f:\n",
    "    conf= yaml.safe_load(f)\n",
    "\n",
    "cds_form_dir=conf['cds_form_dir']\n",
    "datasets_dir = conf['datasets_dir']\n",
    "\n",
    "# datasets_download(datasets_dir)\n",
    "\n",
    "\n",
    "datesbeg = {}\n",
    "datesend = {}\n",
    "ecv_dic = {}\n",
    "for k,ecv in enumerate(conf['ECV']):\n",
    "    print(ecv)\n",
    "    entries = conf['ECV'][ecv]['entry']\n",
    "    print(entries)\n",
    "    datemin_list = []\n",
    "    datemax_list = []\n",
    "    if ecv in ['Earth Radiation Budget']: \n",
    "        datemin,datemax = extract_dates_from_TSI()\n",
    "        datemin_list.append(datemin)\n",
    "        datemax_list.append(datemax)\n",
    "    for entry in entries:\n",
    "        jfilepath=f'{cds_form_dir}{entry}/constraints.json'\n",
    "        print(jfilepath)\n",
    "        if jfilepath == '/Users/cxjo/Documents/cds-forms-c3s/satellite-ice-sheet-elevation-change/constraints.json':\n",
    "            datemin,datemax = extract_dates_icesheets(datasets_dir)\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        elif jfilepath == '/Users/cxjo/Documents/cds-forms-c3s/satellite-ice-sheet-mass-balance/constraints.json':\n",
    "            datemin,datemax = extract_dates_massbalance(datasets_dir)\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        elif jfilepath == '/Users/cxjo/Documents/cds-forms-c3s/derived-gridded-glacier-mass-change//constraints.json':\n",
    "            jfile = f'{cds_form_dir}{entry}/generate.yaml'\n",
    "            datemin,datemax = extract_dates_derived_glaciers(jfile)\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        elif jfilepath == '/Users/cxjo/Documents/cds-forms-c3s/insitu-glaciers-extent/constraints.json':    \n",
    "            datemin= pd.Timestamp('1990-01-01') # http://www.glims.org/rgi_user_guide/06_dataset_summary.html\n",
    "            datemax= pd.Timestamp('2010-12-31')\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        elif jfilepath =='/Users/cxjo/Documents/cds-forms-c3s/satellite-total-column-water-vapour-ocean/constraints.json':\n",
    "            # temporal aggregation is messed up. does not have the same meaning as other datasets\n",
    "            # monthly should be yearly\n",
    "            # 6-hourly should be monthly\n",
    "            # need to write a special function that accounts for this\n",
    "            datemin,datemax = extract_dates_wv(jfilepath)\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        elif jfilepath == '/Users/cxjo/Documents/cds-forms-c3s/satellite-lake-water-level/constraints.json':\n",
    "            datemin,datemax = extract_dates_lake_levels(datasets_dir)\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        # elif jfilepath == '/Users/cxjo/Documents/cds-forms-c3s/satellite-land-cover/constraints.json':\n",
    "        #     datemin_list.append(pd.Timestamp('1992-01-01'))\n",
    "        #     datemax_list.append(pd.Timestamp('2022-12-31'))\n",
    "        else:\n",
    "            # print(jfilepath)\n",
    "            datemin,datemax = calc_dateminmax_from_cds_form(jfilepath,ecv)\n",
    "            # print(ecv,datemin_list,datemax_list)\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "            # print(ecv,datemin_list,datemax_list)\n",
    "\n",
    "    # now get the max and min per ECV, accounting for all products\n",
    "    datemin_list = np.array(datemin_list)\n",
    "    datemax_list = np.array(datemax_list)\n",
    "    # datesbeg[ecv] = np.min(datemin_list)\n",
    "    # datesend[ecv] = np.max(datemax_list)\n",
    "    ecv_dic[k] = {\n",
    "        'ECV'     : ecv,\n",
    "        'DateBeg' : np.min(datemin_list),\n",
    "        'DateEnd' : np.max(datemax_list),\n",
    "        'Thematic Hub' : conf['ECV'][ecv]['Thematic_hub']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | ECV                      | DateBeg    | DateEnd    | Thematic Hub            |\n",
      "|---:|:-------------------------|:-----------|:-----------|:------------------------|\n",
      "|  0 | Aerosols                 | 1995-06-01 | 2023-10-31 | Atmospheric Composition |\n",
      "|  2 | Greenhouse Gases         | 2002-10-01 | 2021-12-31 | Atmospheric Composition |\n",
      "| 14 | Ozone                    | 1970-04-01 | 2023-07-31 | Atmospheric Composition |\n",
      "| 15 | Precipitation            | 1979-01-01 | 2022-06-30 | Atmospheric Physics     |\n",
      "|  8 | Upper-air Water Vapour   | 1988-01-31 | 2023-02-28 | Atmospheric Physics     |\n",
      "| 21 | Surface Radiation Budget | 1979-01-01 | 2023-04-30 | Atmospheric Physics     |\n",
      "|  4 | Earth Radiation Budget   | 1979-01-01 | 2024-03-15 | Atmospheric Physics     |\n",
      "|  3 | Clouds                   | 1979-01-01 | 2023-04-30 | Atmospheric Physics     |\n",
      "|  6 | Ice Sheets               | 1992-01-01 | 2023-08-01 | Cryosphere              |\n",
      "|  7 | Glaciers                 | 1975-04-01 | 2021-09-30 | Cryosphere              |\n",
      "|  9 | LAI                      | 1981-09-20 | 2020-06-30 | Land Biosphere          |\n",
      "| 12 | Land Cover               | 1992-01-01 | 2022-12-31 | Land Biosphere          |\n",
      "|  1 | Albedo                   | 1981-09-20 | 2020-06-30 | Land Biosphere          |\n",
      "|  5 | Fire                     | 2001-01-01 | 2022-04-01 | Land Biosphere          |\n",
      "| 10 | FAPAR                    | 1981-09-20 | 2020-06-30 | Land Biosphere          |\n",
      "| 11 | Lakes                    | 1992-10-13 | 2023-12-31 | Land Hydrology          |\n",
      "| 19 | Soil Moisture            | 1978-11-01 | 2024-03-31 | Land Hydrology          |\n",
      "| 20 | Surface Currents         | 1993-01-01 | 2023-06-07 | Ocean                   |\n",
      "| 13 | Ocean Colour             | 1997-09-04 | 2023-12-31 | Ocean                   |\n",
      "| 16 | Sea Ice                  | 1978-10-25 | 2024-03-04 | Ocean                   |\n",
      "| 17 | Sea Level                | 1993-01-01 | 2023-06-07 | Ocean                   |\n",
      "| 18 | SST                      | 1981-08-24 | 2022-12-31 | Ocean                   |\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type timedelta is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# fig = px.timeline(ecv_pd, x_start=\"DateBeg\", x_end=\"DateEnd\", y='Product',color='Lot')\u001b[39;00m\n\u001b[1;32m     11\u001b[0m fig \u001b[38;5;241m=\u001b[39m px\u001b[38;5;241m.\u001b[39mtimeline(ecv_pd, x_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDateBeg\u001b[39m\u001b[38;5;124m\"\u001b[39m, x_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDateEnd\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mECV\u001b[39m\u001b[38;5;124m'\u001b[39m,color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThematic Hub\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m fig\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# fig = px.timeline(datasets_df, x_start=\"startdate\", x_end=\"enddate\", y='ECV')\u001b[39;00m\n\u001b[1;32m     14\u001b[0m fig\u001b[38;5;241m.\u001b[39mupdate_yaxes(autorange\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreversed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/plotly/basedatatypes.py:3398\u001b[0m, in \u001b[0;36mBaseFigure.show\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3365\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3366\u001b[0m \u001b[38;5;124;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[1;32m   3367\u001b[0m \u001b[38;5;124;03mspecified by the renderer argument\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3394\u001b[0m \u001b[38;5;124;03mNone\u001b[39;00m\n\u001b[1;32m   3395\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3396\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[0;32m-> 3398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pio\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/plotly/io/_renderers.py:388\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m fig_dict \u001b[38;5;241m=\u001b[39m validate_coerce_fig_to_dict(fig, validate)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# Mimetype renderers\u001b[39;00m\n\u001b[0;32m--> 388\u001b[0m bundle \u001b[38;5;241m=\u001b[39m renderers\u001b[38;5;241m.\u001b[39m_build_mime_bundle(fig_dict, renderers_string\u001b[38;5;241m=\u001b[39mrenderer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bundle:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ipython_display:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/plotly/io/_renderers.py:296\u001b[0m, in \u001b[0;36mRenderersConfig._build_mime_bundle\u001b[0;34m(self, fig_dict, renderers_string, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(renderer, k):\n\u001b[1;32m    294\u001b[0m                 \u001b[38;5;28msetattr\u001b[39m(renderer, k, v)\n\u001b[0;32m--> 296\u001b[0m         bundle\u001b[38;5;241m.\u001b[39mupdate(renderer\u001b[38;5;241m.\u001b[39mto_mimebundle(fig_dict))\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bundle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/plotly/io/_base_renderers.py:95\u001b[0m, in \u001b[0;36mPlotlyRenderer.to_mimebundle\u001b[0;34m(self, fig_dict)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config:\n\u001b[1;32m     92\u001b[0m     fig_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m     94\u001b[0m json_compatible_fig_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\n\u001b[0;32m---> 95\u001b[0m     to_json(fig_dict, validate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, remove_uids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     96\u001b[0m )\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/vnd.plotly.v1+json\u001b[39m\u001b[38;5;124m\"\u001b[39m: json_compatible_fig_dict}\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/plotly/io/_json.py:199\u001b[0m, in \u001b[0;36mto_json\u001b[0;34m(fig, validate, pretty, remove_uids, engine)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m trace \u001b[38;5;129;01min\u001b[39;00m fig_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, []):\n\u001b[1;32m    197\u001b[0m         trace\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_json_plotly(fig_dict, pretty\u001b[38;5;241m=\u001b[39mpretty, engine\u001b[38;5;241m=\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/plotly/io/_json.py:123\u001b[0m, in \u001b[0;36mto_json_plotly\u001b[0;34m(plotly_object, pretty, engine)\u001b[0m\n\u001b[1;32m    119\u001b[0m         opts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseparators\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_plotly_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PlotlyJSONEncoder\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mdumps(plotly_object, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39mPlotlyJSONEncoder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopts)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morjson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    125\u001b[0m     JsonConfig\u001b[38;5;241m.\u001b[39mvalidate_orjson()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/json/__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    235\u001b[0m     skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[1;32m    236\u001b[0m     check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[1;32m    237\u001b[0m     separators\u001b[38;5;241m=\u001b[39mseparators, default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys,\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39mencode(obj)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/_plotly_utils/utils.py:59\u001b[0m, in \u001b[0;36mPlotlyJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03mLoad and then dump the result using parse_constant kwarg\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03mNote that setting invalid separators will cause a failure at this step.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# this will raise errors in a normal-expected way\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m encoded_o \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(PlotlyJSONEncoder, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mencode(o)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Brute force guessing whether NaN or Infinity values are in the string\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# We catch false positive cases (e.g. strings such as titles, labels etc.)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# but this is ok since the intention is to skip the decoding / reencoding\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# step when it's completely safe\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m encoded_o \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInfinity\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m encoded_o):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/json/encoder.py:200\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterencode(o, _one_shot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    202\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/json/encoder.py:258\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[1;32m    255\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _iterencode(o, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/_plotly_utils/utils.py:136\u001b[0m, in \u001b[0;36mPlotlyJSONEncoder.default\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m NotEncodable:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _json\u001b[38;5;241m.\u001b[39mJSONEncoder\u001b[38;5;241m.\u001b[39mdefault(\u001b[38;5;28mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/json/encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    181\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type timedelta is not JSON serializable"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ecv_pd = pd.DataFrame([conf['ECV'].keys(),datesbeg,datesend],index=['DateBeg','DateEnd']).T\n",
    "ecv_pd = pd.DataFrame.from_dict(ecv_dic,orient='index').sort_values(['Thematic Hub'])\n",
    "ecv_pd['DateBeg'] = ecv_pd['DateBeg'].dt.ceil(freq='s')  \n",
    "ecv_pd['DateEnd'] = ecv_pd['DateEnd'].dt.ceil(freq='s')  \n",
    "ecv_pd['DateEnd'] = ecv_pd['DateEnd'].apply(lambda dt: dt.strftime(\"%Y-%m-%d\"))\n",
    "ecv_pd['DateBeg'] = ecv_pd['DateBeg'].apply(lambda dt: dt.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "print(ecv_pd.to_markdown())\n",
    "\n",
    "# fig = px.timeline(ecv_pd, x_start=\"DateBeg\", x_end=\"DateEnd\", y='Product',color='Lot')\n",
    "fig = px.timeline(ecv_pd, x_start=\"DateBeg\", x_end=\"DateEnd\",y='ECV',color='Thematic Hub')\n",
    "\n",
    "# fig = px.timeline(datasets_df, x_start=\"startdate\", x_end=\"enddate\", y='ECV')\n",
    "fig.update_yaxes(autorange=\"reversed\")\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1200,\n",
    "    height=800,\n",
    ")\n",
    "# fig.update_layout(\n",
    "#     xaxis = dict(\n",
    "#         dtick = 'Y1',\n",
    "#         tickformat=\"%Y\",\n",
    "#     )\n",
    "# )\n",
    "\n",
    "xlab = np.arange(1970,2025).astype('int')\n",
    "xlabtxt = [f'{i}' for i in xlab]\n",
    "\n",
    "\n",
    "fig.update_xaxes(minor=dict(ticks=\"inside\", showgrid=True))\n",
    "fig.update_layout(\n",
    "    xaxis = dict(\n",
    "        tickmode = 'array',\n",
    "        tickvals = xlab,\n",
    "        ticktext = xlabtxt\n",
    "    )\n",
    ")\n",
    "fig.update_xaxes(tickangle=-45)\n",
    "fig.update_layout(\n",
    "    xaxis = dict(\n",
    "        tickfont = dict(\n",
    "            size=10),\n",
    "        )\n",
    "    )\n",
    "fig.update_xaxes(range = ['1970-01-01','2024-12-31'])\n",
    "# print(fig)\n",
    "# today_date = pd.Timestamp.today().strftime('%Y%m%d') \n",
    "# print(today_date)\n",
    "# fig.write_image(f'temporal_coverage_by_ECV_{today_date}.pdf')\n",
    "# fig.write_image(f'temporal_coverage_by_ECV_{today_date}.png')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datemin: 1992-01-01 00:00:00 datemax: 2023-07-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
