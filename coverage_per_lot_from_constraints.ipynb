{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import os\n",
    "from datetime import datetime,date\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import json\n",
    "import yaml\n",
    "from calendar import monthrange\n",
    "import xarray as xr\n",
    "import cdsapi\n",
    "import zipfile\n",
    "from glob import glob\n",
    "import datetime as dt     \n",
    "\n",
    "\n",
    "#important\n",
    "#before running this script, please go to cads-forms-json, git pull, then git checkout prod\n",
    "\n",
    "\n",
    "# wishlist\n",
    "# - Ice sheet velocity to include antarctica\n",
    "# - put land hydrology just below cryosphere\n",
    "# - update repository cds-forms-c3s and change to c3sprod\n",
    "#   - git checkout c3sprod\n",
    "#   - git pull\n",
    "# - download datasets that only contain info in the time dimension from CDS\n",
    "# - try to make the code more path-independent\n",
    "# - install on athos and run every week. update some figure repo\n",
    "\n",
    "\n",
    "def extract_dates_from_TSI():\n",
    "    url='https://gerb.oma.be/tsi/C3S_RMIB_daily_TSI_composite_ICDR_v3_latest.txt'\n",
    "    c=pd.read_csv(url,skiprows=128,delim_whitespace=True,header=None)\n",
    "    return pd.Timestamp(str(c[3].iloc[0])),pd.Timestamp(str(c[3].iloc[-1]))\n",
    "def extract_dates_icesheets(datasets_dir, region):\n",
    "    # opens TCDR and ICDR files for both Antarctica and Greenland, then computes max/min dates\n",
    "    # needs adjustment for per product sorting\n",
    "    # region: Ant, Gr\n",
    "    \n",
    "    # -- Ice Sheet Surface Elevation Change files\n",
    "    cads_forms_yml_dir=conf['cads_forms_yml_dir']\n",
    "    product='Ice Sheet Surface Elevation Change (Antarctica)' # contains all info for IS-SEC\n",
    "    entry=conf['PRODUCT'][product]['entry'][0]\n",
    "    fname_generate=f'{cads_forms_yml_dir}/{entry}/gecko-config/generate.yaml'\n",
    "    with open(fname_generate) as f:\n",
    "        generate= yaml.safe_load(f)\n",
    "    # print(generate['manifest'])\n",
    "\n",
    "    #download all manifest files\n",
    "    os.system(f'rm -rf {datasets_dir}/manifest*')\n",
    "    for i in range(len(generate['manifest'])):\n",
    "        os.system(f'wget {generate['manifest'][i]} -P {datasets_dir}')\n",
    "\n",
    "    # download latest versions if needed\n",
    "    if region in ['Ant']:\n",
    "        dsets = ['AntIS_CDR','AntIS_ICDR']\n",
    "    elif region in ['Gr']:\n",
    "        dsets= ['GrIS_CDR','GrIS_CDR']\n",
    "    else:\n",
    "        print('please set the region correctly for Ice Sheet SEC!!')\n",
    "        raise SystemExit\n",
    "    for dset in dsets:\n",
    "        fname_manif=glob(f'{datasets_dir}/*{dset}_latest.txt')[0]\n",
    "        print(fname_manif)\n",
    "        with open(fname_manif) as f:\n",
    "            flist=f.readlines()\n",
    "        fname_gris=flist[-1].replace('\\n','').split('/')[-1]\n",
    "        if os.path.exists(f'{datasets_dir}{fname_gris}'):\n",
    "            print(f'{fname_gris} file exists!')\n",
    "        else:\n",
    "            print(f'Downloading {fname_gris} ...')\n",
    "            os.system(f'wget {flist[-1].replace('\\n','')} -P {datasets_dir}')\n",
    "\n",
    "    if region in ['Gr']: fnames = glob(datasets_dir+'C3S_*GrIS_RA*.nc')\n",
    "    if region in ['Ant']: fnames = glob(datasets_dir+'C3S_*AntIS_RA*.nc')\n",
    "    print(fnames)\n",
    "    datebegs = []\n",
    "    dateends = []\n",
    "    for fname in fnames:\n",
    "        print('will open '+fname)\n",
    "        nc = xr.open_dataset(fname)\n",
    "        datebegs.append(pd.Timestamp(nc['time'].values[0]))\n",
    "        dateends.append(pd.Timestamp(nc['time'].values[-1]))\n",
    "    return min(datebegs),max(dateends)\n",
    "def extract_dates_massbalance(datasets_dir):\n",
    "    # -- Ice Sheet Gravimetric Mass Balance\n",
    "    cads_forms_yml_dir=conf['cads_forms_yml_dir']\n",
    "    product='Ice Sheet Gravimetric Mass Balance' # contains all info for IS-SEC\n",
    "    entry=conf['PRODUCT'][product]['entry'][0]\n",
    "    fname_generate=f'{cads_forms_yml_dir}/{entry}/gecko-config/generate.yaml'\n",
    "    with open(fname_generate) as f:\n",
    "        generate= yaml.safe_load(f)\n",
    "    \n",
    "    #download all manifest files\n",
    "    os.system(f'rm -rf {datasets_dir}/manifest*')\n",
    "    for i in range(len(generate['manifest'])):\n",
    "        os.system(f'wget {generate['manifest'][i]} -P {datasets_dir}')\n",
    "    \n",
    "    # download latest versions if needed\n",
    "    fname_manif=glob(f'{datasets_dir}/*gravimetry_latest.txt')[0]\n",
    "    print(fname_manif)\n",
    "    with open(fname_manif) as f:\n",
    "        flist=f.readlines()\n",
    "    fname_gmb=flist[-1].replace('\\n','').split('/')[-1]\n",
    "    if os.path.exists(f'{datasets_dir}{fname_gmb}'):\n",
    "        print(f'{fname_gmb} file exists!')\n",
    "    else:\n",
    "        print(f'Downloading {fname_gmb} ...')\n",
    "        os.system(f'wget {flist[-1].replace('\\n','')} -P {datasets_dir}')\n",
    "    \n",
    "    \n",
    "    fname = f'{datasets_dir}{fname_gmb}'\n",
    "    nc = xr.open_dataset(fname)\n",
    "    return pd.Timestamp(nc['time'].values[0]),pd.Timestamp(nc['time'].values[-1])\n",
    "def extract_dates_derived_glaciers(jfile):\n",
    "    with open(jfile) as f:\n",
    "        gen= yaml.safe_load(f)[0]\n",
    "    ymin = int(gen['hydrological_year'][0][0:4])\n",
    "    ymax = int(gen['hydrological_year'][-1][0:4])+1\n",
    "    mmax=9\n",
    "    dmax=30\n",
    "    mmin=4\n",
    "    dmin=1\n",
    "    print('Glaciers ',ymin,ymax)\n",
    "    return pd.Timestamp(f'{ymin}-{mmin}-{dmin}'), pd.Timestamp(f'{ymax}-{mmax}-{dmax}')\n",
    "def extract_dates_lake_levels(datasets_dir):\n",
    "    cads_forms_yml_dir=conf['cads_forms_yml_dir']\n",
    "    product='Lake Water Level' # contains all info for IS-SEC\n",
    "    entry=conf['PRODUCT'][product]['entry'][0]\n",
    "    fname_generate=f'{cads_forms_yml_dir}/{entry}/gecko-config/generate.yaml'\n",
    "    with open(fname_generate) as f:\n",
    "        generate= yaml.safe_load(f)\n",
    "    # print(generate['manifest'])\n",
    "\n",
    "    #download all manifest files\n",
    "    os.system(f'rm -rf {datasets_dir}/manifest*')\n",
    "    os.system(f'wget {generate['manifest'][-1]} -P {datasets_dir}')\n",
    "\n",
    "    fname_manif=glob(f'{datasets_dir}/manifest_*lakes_lwl_latest.txt')[0]\n",
    "    print(fname_manif)\n",
    "\n",
    "    #load manifest files and retrieve time coverage from the different filenames\n",
    "    with open(fname_manif) as f:\n",
    "        flist=f.readlines()\n",
    "    datebeg=min([pd.to_datetime(flist[i].replace('\\n','').split('/')[-1].split('_')[-3]) for i in range(len(flist))])\n",
    "    dateend=max([pd.to_datetime(flist[i].replace('\\n','').split('/')[-1].split('_')[-2]) for i in range(len(flist))])\n",
    "    return pd.Timestamp(datebeg),pd.Timestamp(dateend)\n",
    "def datemax2(row):\n",
    "    row = row.dropna()\n",
    "    ymax = int(max(row['year']))\n",
    "    if 'month' in row.keys():\n",
    "        mmax = int(max(row['month']))\n",
    "    else:\n",
    "        mmax=12\n",
    "    xx,dmax=monthrange(ymax,mmax)\n",
    "    datemax = pd.Timestamp(f'{ymax}-{mmax}-{dmax}')\n",
    "    return datemax\n",
    "def datemin2(row):\n",
    "    row = row.dropna()\n",
    "    ymin = int(min(row['year']))\n",
    "    if 'month' in row.keys():\n",
    "        mmin = int(min(row['month']))\n",
    "    else:\n",
    "        mmin=1\n",
    "    xx,dmin=monthrange(ymin,mmin)\n",
    "    datemin = pd.Timestamp(f'{ymin}-{mmin}-{dmin}')\n",
    "    return datemin\n",
    "def extract_dates_wv(jfilepath):\n",
    "    f = open(jfilepath)\n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    data = json.load(f)\n",
    "    # print(data)\n",
    "    df = pd.DataFrame(data)\n",
    "    df['datemax'] = df.apply(datemax2,axis=1)\n",
    "    df['datemin'] = df.apply(datemin2,axis=1)\n",
    "    datemin = df['datemin'].min()\n",
    "    datemax = df['datemax'].max()\n",
    "    return datemin,datemax \n",
    "def check_time_agg(row):\n",
    "    row = row.dropna()\n",
    "    if 'time_aggregation' in row.keys():\n",
    "        time_aggregation = row['time_aggregation'][0]\n",
    "        # print('time_agg',time_aggregation)\n",
    "        if time_aggregation in ['daily_average','daily_mean','day','day_average']: \n",
    "            time_agg = 'day'\n",
    "            return time_agg\n",
    "        elif time_aggregation == 'daily':\n",
    "            time_agg='daily'\n",
    "            return time_agg\n",
    "        #note interim solution for 5-daily-composite...\n",
    "        elif time_aggregation in [\n",
    "            'monthly_average',\n",
    "            '5_daily_composite',\n",
    "            'monthly_mean',\n",
    "            '27_days',\n",
    "            'month',\n",
    "            'monthly',\n",
    "            'month_average',\n",
    "            '10_day_average', # debatable..\n",
    "            ]:\n",
    "            time_agg = 'monthly'\n",
    "            return time_agg\n",
    "        else:\n",
    "            # print(row)\n",
    "            print('Could not determine time_agg')\n",
    "            raise SystemExit\n",
    "    elif 'period' in row.keys(): # applies to ice_sheets\n",
    "        time_agg = 'period'\n",
    "        return time_agg\n",
    "    elif 'temporal_aggregation' in row.keys():\n",
    "        time_aggregation = row['temporal_aggregation'][0]\n",
    "        if time_aggregation in ['monthly','6-hourly']:\n",
    "            time_agg='monthly'\n",
    "        elif time_aggregation == 'daily':\n",
    "            time_agg='daily'\n",
    "        else:\n",
    "            print('Error in temporal aggregation')\n",
    "            raise SystemExit\n",
    "        return time_agg\n",
    "    else:\n",
    "        if ('day' in row.keys()):\n",
    "            time_agg='day'\n",
    "            return time_agg\n",
    "        elif  'nominal_day' in row.keys():\n",
    "            time_agg='nominal_day'\n",
    "            return time_agg\n",
    "        else:\n",
    "            time_agg = 'monthly'\n",
    "            return time_agg\n",
    "def compute_datemax(row):\n",
    " \n",
    "    time_agg=check_time_agg(row) # check time aggregation of data in this row\n",
    "    # print('time_agg',time_agg)\n",
    "    if time_agg =='period':\n",
    "        per_str=max(row['period'])\n",
    "        ymax=int(per_str[5::])\n",
    "        mmax=9\n",
    "        dmax=30\n",
    "    else: \n",
    "        ymax = int(max(row['year']))\n",
    "        if 'month' in row.keys():\n",
    "            mmax = int(max(row['month']))\n",
    "        else:\n",
    "            mmax=12\n",
    "        xx,ndays = monthrange(ymax,mmax)\n",
    "        if time_agg in ['day','nominal_day']:\n",
    "            # print(row[time_agg])\n",
    "            dmax = int(max(row[time_agg])) \n",
    "        elif time_agg in ['daily']:\n",
    "            dmax = int(max(row['day'])) \n",
    "        else:\n",
    "            dmax = ndays # last day of month\n",
    "        if dmax >ndays:\n",
    "            print('Beware error in allowed dates...')\n",
    "            # print(row)\n",
    "            dmax=ndays\n",
    "\n",
    "    datemax = pd.Timestamp(f'{ymax}-{mmax}-{dmax}')\n",
    "    return datemax\n",
    "def compute_datemin(row):\n",
    "    time_agg=check_time_agg(row) # check time aggregation of data in this row\n",
    "    if time_agg =='period':\n",
    "        per_str=max(row['period'])\n",
    "        ymin=int(per_str[0:4])\n",
    "        mmin=10\n",
    "        dmin=1\n",
    "    else:\n",
    "        ymin = int(min(row['year'])) \n",
    "        if 'month' in row.keys():\n",
    "            mmin = int(min(row['month']))\n",
    "        else:\n",
    "            mmin=1\n",
    "        if time_agg in ['day','nominal_day']:\n",
    "            dmin = int(min(row[time_agg])) \n",
    "        elif time_agg in ['daily']:\n",
    "            dmin = int(min(row['day'])) \n",
    "        else:\n",
    "            dmin = 1 # first day of month\n",
    "        xx,ndays = monthrange(ymin,mmin)     \n",
    "        if dmin>ndays:\n",
    "            print('Beware error in allowed dates...')\n",
    "            # print(row)\n",
    "            dmin=1\n",
    "    datemin = pd.Timestamp(f'{ymin}-{mmin}-{dmin}')\n",
    "    return datemin\n",
    "def calc_dateminmax_from_cds_form(jfilepath,ecv):\n",
    "    # Opening JSON file\n",
    "    f = open(jfilepath)\n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    data = json.load(f)\n",
    "    # print(data)\n",
    "    df = pd.DataFrame(data)\n",
    "    # display(df)\n",
    "    # print(df.keys())\n",
    "    # print(len(df))\n",
    "    \n",
    "    # find records where dates cannot be defined\n",
    "    if 'sensor_and_algorithm' in df.keys():        \n",
    "        lst_erase=[]\n",
    "        for i in range(len(df)):\n",
    "            if (df['sensor_and_algorithm'][i][0]=='merged_obs4mips'): lst_erase.append(i)\n",
    "        # now .drop these problematic rows\n",
    "        for i in lst_erase:\n",
    "            df=df.drop(lst_erase)\n",
    "    if ecv == 'Earth Radiation Budget':        \n",
    "        lst_erase=[]\n",
    "        for i in range(len(df)):\n",
    "            if (df['variable'][i][0]=='total_solar_irradiance'): lst_erase.append(i) # this info is read from the dataset itself\n",
    "        # now .drop these problematic rows\n",
    "        for i in lst_erase:\n",
    "            df=df.drop(lst_erase)\n",
    "    # for i in range(len(df)):\n",
    "    #     print(df.loc[i])\n",
    "    #     if ('year' not in df[i]): lst_erase.append(i)\n",
    "\n",
    "    df['datemax'] = df.apply(compute_datemax,axis=1)\n",
    "    df['datemin'] = df.apply(compute_datemin,axis=1)\n",
    "    datemin = df['datemin'].min()\n",
    "    datemax = df['datemax'].max()\n",
    "    return datemin,datemax\n",
    "\n",
    "with open('config-athos.yml') as f:\n",
    "    conf= yaml.safe_load(f)\n",
    "\n",
    "cds_form_dir=conf['cds_form_dir']\n",
    "datasets_dir = conf['datasets_dir']\n",
    "thisyear=dt.datetime.today().year\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILD PANDAS DATAFRAME FOR TIME COVERAGE BY ECV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datesbeg = {}\n",
    "datesend = {}\n",
    "ecv_dic = {}\n",
    "for k,ecv in enumerate(conf['ECV']):\n",
    "    print(ecv)\n",
    "    entries = conf['ECV'][ecv]['entry']\n",
    "    print(entries)\n",
    "    datemin_list = []\n",
    "    datemax_list = []\n",
    "    if ecv in ['Earth Radiation Budget']: \n",
    "        datemin,datemax = extract_dates_from_TSI()\n",
    "        datemin_list.append(datemin)\n",
    "        datemax_list.append(datemax)\n",
    "    for entry in entries:\n",
    "        jfilepath=f'{cds_form_dir}{entry}/constraints.json'\n",
    "        print(jfilepath)\n",
    "        if entry == 'satellite-ice-sheet-elevation-change':\n",
    "            datemin,datemax = extract_dates_icesheets(datasets_dir,'Ant')\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "            datemin,datemax = extract_dates_icesheets(datasets_dir,'Gr')\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        elif entry == 'satellite-ice-sheet-mass-balance':\n",
    "            datemin,datemax = extract_dates_massbalance(datasets_dir)\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        elif entry == 'derived-gridded-glacier-mass-change':\n",
    "            jfile = f'{cds_form_dir}{entry}/constraints.json'\n",
    "            datemin,datemax = extract_dates_derived_glaciers(jfile)\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        elif entry == 'insitu-glaciers-extent':    \n",
    "            datemin= pd.Timestamp('1990-01-01') # http://www.glims.org/rgi_user_guide/06_dataset_summary.html\n",
    "            datemax= pd.Timestamp('2010-12-31')\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        elif entry =='satellite-total-column-water-vapour-ocean':\n",
    "            # temporal aggregation is messed up. does not have the same meaning as other datasets\n",
    "            # monthly should be yearly\n",
    "            # 6-hourly should be monthly\n",
    "            # need to write a special function that accounts for this\n",
    "            datemin,datemax = extract_dates_wv(jfilepath)\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        elif entry == 'satellite-lake-water-level':\n",
    "            datemin,datemax = extract_dates_lake_levels(datasets_dir)\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        # elif jfilepath == '/Users/cxjo/Documents/cds-forms-c3s/satellite-land-cover/constraints.json':\n",
    "        #     datemin_list.append(pd.Timestamp('1992-01-01'))\n",
    "        #     datemax_list.append(pd.Timestamp('2022-12-31'))\n",
    "        else:\n",
    "            # print(jfilepath)\n",
    "            datemin,datemax = calc_dateminmax_from_cds_form(jfilepath,ecv)\n",
    "            # print(ecv,datemin_list,datemax_list)\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "            # print(ecv,datemin_list,datemax_list)\n",
    "\n",
    "    # now get the max and min per ECV, accounting for all products\n",
    "    datemin_list = np.array(datemin_list)\n",
    "    datemax_list = np.array(datemax_list)\n",
    "    # datesbeg[ecv] = np.min(datemin_list)\n",
    "    # datesend[ecv] = np.max(datemax_list)\n",
    "    ecv_dic[k] = {\n",
    "        'ECV'     : ecv,\n",
    "        'DateBeg' : np.min(datemin_list),\n",
    "        'DateEnd' : np.max(datemax_list),\n",
    "        'Thematic Hub' : conf['ECV'][ecv]['Thematic_hub']\n",
    "    }\n",
    "\n",
    "\n",
    "# ecv_pd = pd.DataFrame([conf['ECV'].keys(),datesbeg,datesend],index=['DateBeg','DateEnd']).T\n",
    "ecv_pd = pd.DataFrame.from_dict(ecv_dic,orient='index').sort_values(['Thematic Hub'])\n",
    "ecv_pd['DateBeg'] = ecv_pd['DateBeg'].dt.ceil(freq='s')  \n",
    "ecv_pd['DateEnd'] = ecv_pd['DateEnd'].dt.ceil(freq='s')  \n",
    "ecv_pd['DateEnd'] = ecv_pd['DateEnd'].apply(lambda dt: dt.strftime(\"%Y-%m-%d\"))\n",
    "ecv_pd['DateBeg'] = ecv_pd['DateBeg'].apply(lambda dt: dt.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "print(ecv_pd.to_markdown())\n",
    "ecv_pd.to_excel('ECV_time_coverage_perECV.xlsx')\n",
    "# fig = px.timeline(ecv_pd, x_start=\"DateBeg\", x_end=\"DateEnd\", y='Product',color='Lot')\n",
    "fig = px.timeline(ecv_pd, x_start=\"DateBeg\", x_end=\"DateEnd\",y='ECV',color='Thematic Hub')\n",
    "\n",
    "# fig = px.timeline(datasets_df, x_start=\"startdate\", x_end=\"enddate\", y='ECV')\n",
    "fig.update_yaxes(autorange=\"reversed\")\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1200,\n",
    "    height=800,\n",
    ")\n",
    "# fig.update_layout(\n",
    "#     xaxis = dict(\n",
    "#         dtick = 'Y1',\n",
    "#         tickformat=\"%Y\",\n",
    "#     )\n",
    "# )\n",
    "\n",
    "xlab = np.arange(1970,thisyear).astype('int')\n",
    "xlabtxt = [f'{i}' for i in xlab]\n",
    "\n",
    "\n",
    "fig.update_xaxes(minor=dict(ticks=\"inside\", showgrid=True))\n",
    "fig.update_layout(\n",
    "    xaxis = dict(\n",
    "        tickmode = 'array',\n",
    "        tickvals = xlab,\n",
    "        ticktext = xlabtxt\n",
    "    )\n",
    ")\n",
    "fig.update_xaxes(tickangle=-45)\n",
    "fig.update_layout(\n",
    "    xaxis = dict(\n",
    "        tickfont = dict(\n",
    "            size=10),\n",
    "        )\n",
    "    )\n",
    "fig.update_xaxes(range = ['1970-01-01',f'{thisyear}-12-31'])\n",
    "# print(fig)\n",
    "today_date = pd.Timestamp.today().strftime('%Y%m%d') \n",
    "print(today_date)\n",
    "fig.write_image(f'temporal_coverage_by_ECV_{today_date}.pdf')\n",
    "fig.write_image(f'temporal_coverage_by_ECV_{today_date}.png')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build pandas dataframe for the figure ordered by product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dateminmax_from_cds_form_2(jfilepath,ecv):\n",
    "    # Opening JSON file\n",
    "    f = open(jfilepath)\n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    data = json.load(f)\n",
    "    # print(data)\n",
    "    df = pd.DataFrame(data)\n",
    "    # display(df)\n",
    "    \n",
    "    # print(df.keys())\n",
    "    # print(len(df))\n",
    "    \n",
    "    # find records where dates cannot be defined\n",
    "    if 'sensor_and_algorithm' in df.keys():        \n",
    "        lst_erase=[]\n",
    "        for i in range(len(df)):\n",
    "            if (df['sensor_and_algorithm'][i][0]=='merged_obs4mips'): lst_erase.append(i)\n",
    "        # now .drop these problematic rows\n",
    "        for i in lst_erase:\n",
    "            df=df.drop(lst_erase)\n",
    "    if ecv == 'Earth Radiation Budget':        \n",
    "        lst_erase=[]\n",
    "        for i in range(len(df)):\n",
    "            if (df['variable'][i][0]=='total_solar_irradiance'): lst_erase.append(i) # this info is read from the dataset itself\n",
    "        # now .drop these problematic rows\n",
    "        for i in lst_erase:\n",
    "            df=df.drop(lst_erase)\n",
    "    # for i in range(len(df)):\n",
    "    #     print(df.loc[i])\n",
    "    #     if ('year' not in df[i]): lst_erase.append(i)\n",
    "\n",
    "    df['datemax'] = df.apply(compute_datemax,axis=1)\n",
    "    df['datemin'] = df.apply(compute_datemin,axis=1)\n",
    "\n",
    "    datemin = df['datemin'].min()\n",
    "    datemax = df['datemax'].max()\n",
    "\n",
    "    return datemin,datemax\n",
    "def extract_dates_dwd_products(jfilepath,product_family):\n",
    "    \n",
    "    f = open(jfilepath)\n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    data = json.load(f)\n",
    "    # print(data)\n",
    "    df = pd.DataFrame(data)\n",
    "    df_temp=df.copy()\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i,'product_family']= df_temp['product_family'][i][0]\n",
    "    df2 = df[df['product_family'] ==product_family]\n",
    "    # display(df)\n",
    "    # display(df2)\n",
    "    df2['datemax'] = df2.apply(compute_datemax,axis=1)\n",
    "    df2['datemin'] = df2.apply(compute_datemin,axis=1)\n",
    "\n",
    "    datemin = df2['datemin'].min()\n",
    "    datemax = df2['datemax'].max()\n",
    "\n",
    "    return datemin, datemax\n",
    "\n",
    "prod_dic = {}\n",
    "datemax_list = []\n",
    "datemin_list = []\n",
    "\n",
    "for k,prod in enumerate(conf['PRODUCT']):\n",
    "    # print(prod, conf['PRODUCT'][prod]['ECV'])\n",
    "    ecv = conf['PRODUCT'][prod]['ECV']\n",
    "    entry = conf['PRODUCT'][prod]['entry'][0]\n",
    "    product = conf['PRODUCT'][prod]['Product']\n",
    "    themHub = conf['PRODUCT'][prod]['Thematic_hub']\n",
    "    jfilepath=f'{cds_form_dir}{entry}/constraints.json'\n",
    "    print(jfilepath,prod)\n",
    "    if prod == 'ERB_RMIB_TSI':\n",
    "        datemin,datemax = extract_dates_from_TSI()\n",
    "    elif prod in ['CLOUDS_CLARA-A2','CLOUDS_CLARA-A3','CLOUDS_CCI_C3S',\n",
    "                  'ERB_NASA_CERES','ERB_NOAA_HIRS','ERB_CCI_C3S','ERB_CLARA-A3',\n",
    "                  'SRB_CLARA-A2','SRB_CLARA-A3','SRB_CCI_C3S']:\n",
    "        product_family=conf['PRODUCT'][prod]['product_family']\n",
    "        datemin,datemax = extract_dates_dwd_products(jfilepath,product_family=product_family)    \n",
    "    elif prod == 'Ice Sheet Gravimetric Mass Balance':\n",
    "        fname = glob(datasets_dir+'C3S_GMB*')[0]\n",
    "        nc = xr.open_dataset(fname)\n",
    "        datemin,datemax=(pd.Timestamp(nc['time'].values[0]),pd.Timestamp(nc['time'].values[-1]))\n",
    "    elif prod == 'Ice Sheet Surface Elevation Change (Antarctica)':\n",
    "        datemin,datemax = extract_dates_icesheets(datasets_dir,'Ant')\n",
    "    elif prod == 'Ice Sheet Surface Elevation Change (Greenland)':\n",
    "        datemin,datemax = extract_dates_icesheets(datasets_dir,'Gr')\n",
    "    elif prod == 'Glaciers elevation and mass change data':\n",
    "        jfile = f'{cds_form_dir}{entry}/constraints.json'\n",
    "        print(jfile)\n",
    "        datemin,datemax = extract_dates_derived_glaciers(jfile)\n",
    "        datemin_list.append(datemin)\n",
    "        datemax_list.append(datemax)\n",
    "    elif entry == 'insitu-glaciers-extent':    \n",
    "        datemin= pd.Timestamp('1990-01-01') # http://www.glims.org/rgi_user_guide/06_dataset_summary.html\n",
    "        datemax= pd.Timestamp('2010-12-31')\n",
    "        datemin_list.append(datemin)\n",
    "        datemax_list.append(datemax)\n",
    "    elif entry =='satellite-total-column-water-vapour-ocean':\n",
    "        # temporal aggregation is messed up. does not have the same meaning as other datasets\n",
    "        # monthly should be yearly\n",
    "        # 6-hourly should be monthly\n",
    "        # need to write a special function that accounts for this\n",
    "        datemin,datemax = extract_dates_wv(jfilepath)\n",
    "        datemin_list.append(datemin)\n",
    "        datemax_list.append(datemax)\n",
    "    elif entry == 'satellite-lake-water-level':\n",
    "        datemin,datemax = extract_dates_lake_levels(datasets_dir)\n",
    "        datemin_list.append(datemin)\n",
    "        datemax_list.append(datemax)\n",
    "    else:\n",
    "        datemin,datemax = calc_dateminmax_from_cds_form_2(jfilepath,ecv)\n",
    "    print(prod,datemin,datemax)\n",
    "    datemin_list.append(datemin)\n",
    "    datemax_list.append(datemax)\n",
    "    \n",
    "    prod_dic[k] = {\n",
    "    'Product': product,\n",
    "    'ECV'     : ecv,\n",
    "    'DateBeg' : datemin,\n",
    "    'DateEnd' : datemax,\n",
    "    'Thematic Hub' : themHub\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ecv_pd = pd.DataFrame([conf['ECV'].keys(),datesbeg,datesend],index=['DateBeg','DateEnd']).T\n",
    "prod_pd = pd.DataFrame.from_dict(prod_dic,orient='index').sort_values(['Thematic Hub','ECV','Product'])\n",
    "prod_pd['DateBeg'] = prod_pd['DateBeg'].dt.ceil(freq='s')  \n",
    "prod_pd['DateEnd'] = prod_pd['DateEnd'].dt.ceil(freq='s')  \n",
    "prod_pd['DateEnd'] = prod_pd['DateEnd'].apply(lambda dt: dt.strftime(\"%Y-%m-%d\"))\n",
    "prod_pd['DateBeg'] = prod_pd['DateBeg'].apply(lambda dt: dt.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "print(prod_pd.to_markdown())\n",
    "prod_pd.to_excel('ECV_time_coverage_perProduct.xlsx')\n",
    "\n",
    "# fig = px.timeline(ecv_pd, x_start=\"DateBeg\", x_end=\"DateEnd\", y='Product',color='Lot')\n",
    "fig = px.timeline(prod_pd, x_start=\"DateBeg\", x_end=\"DateEnd\",y='Product',color='Thematic Hub')\n",
    "\n",
    "# fig = px.timeline(datasets_df, x_start=\"startdate\", x_end=\"enddate\", y='ECV')\n",
    "fig.update_yaxes(autorange=\"reversed\")\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1200,\n",
    "    height=800,\n",
    ")\n",
    "# fig.update_layout(\n",
    "#     xaxis = dict(\n",
    "#         dtick = 'Y1',\n",
    "#         tickformat=\"%Y\",\n",
    "#     )\n",
    "# )\n",
    "\n",
    "xlab = np.arange(1970,thisyear).astype('int')\n",
    "xlabtxt = [f'{i}' for i in xlab]\n",
    "\n",
    "\n",
    "fig.update_xaxes(minor=dict(ticks=\"inside\", showgrid=True))\n",
    "fig.update_layout(\n",
    "    xaxis = dict(\n",
    "        tickmode = 'array',\n",
    "        tickvals = xlab,\n",
    "        ticktext = xlabtxt\n",
    "    )\n",
    ")\n",
    "fig.update_xaxes(tickangle=-45)\n",
    "fig.update_layout(\n",
    "    xaxis = dict(\n",
    "        tickfont = dict(\n",
    "            size=10),\n",
    "        )\n",
    "    )\n",
    "fig.update_xaxes(range = ['1970-01-01',f'{thisyear}-12-31'])\n",
    "# print(fig)\n",
    "today_date = pd.Timestamp.today().strftime('%Y%m%d') \n",
    "print(today_date)\n",
    "fig.write_image(f'temporal_coverage_by_Product_{today_date}.pdf')\n",
    "fig.write_image(f'temporal_coverage_by_Product_{today_date}.png')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
