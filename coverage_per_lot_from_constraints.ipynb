{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import os\n",
    "from datetime import datetime,date\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import json\n",
    "import yaml\n",
    "from calendar import monthrange\n",
    "import xarray as xr\n",
    "import cdsapi\n",
    "import zipfile\n",
    "from glob import glob\n",
    "import datetime as dt     \n",
    "\n",
    "\n",
    "#important\n",
    "#before running this script, please go to cads-forms-json, git pull, then git checkout prod\n",
    "\n",
    "\n",
    "# wishlist\n",
    "# - Ice sheet velocity to include antarctica\n",
    "# - put land hydrology just below cryosphere\n",
    "# - update repository cds-forms-c3s and change to c3sprod\n",
    "#   - git checkout c3sprod\n",
    "#   - git pull\n",
    "# - download datasets that only contain info in the time dimension from CDS\n",
    "# - try to make the code more path-independent\n",
    "# - install on athos and run every week. update some figure repo\n",
    "\n",
    "\n",
    "def extract_dates_from_TSI():\n",
    "    url='https://gerb.oma.be/tsi/C3S_RMIB_daily_TSI_composite_ICDR_v3.4.txt' # check this URL everytime\n",
    "    c=pd.read_csv(url,skiprows=128,delim_whitespace=True,header=None)\n",
    "    return pd.Timestamp(str(c[3].iloc[0])),pd.Timestamp(str(c[3].iloc[-1]))\n",
    "def extract_dates_icesheets(datasets_dir, region):\n",
    "    # opens TCDR and ICDR files for both Antarctica and Greenland, then computes max/min dates\n",
    "    # needs adjustment for per product sorting\n",
    "    # region: Ant, Gr\n",
    "    \n",
    "    # -- Ice Sheet Surface Elevation Change files\n",
    "    cads_forms_yml_dir=conf['cads_forms_yml_dir']\n",
    "    product='Ice Sheet Surface Elevation Change (Antarctica)' # contains all info for IS-SEC\n",
    "    entry=conf['PRODUCT'][product]['entry'][0]\n",
    "    fname_generate=f'{cads_forms_yml_dir}/{entry}/gecko-config/generate.yaml'\n",
    "    with open(fname_generate) as f:\n",
    "        generate= yaml.safe_load(f)\n",
    "    # print(generate['manifest'])\n",
    "\n",
    "    #download all manifest files\n",
    "    os.system(f'rm -rf {datasets_dir}/*manifest*')\n",
    "    # for i in range(len(generate['manifest'])):\n",
    "    #     os.system(f'wget {generate['manifest'][i]} -P {datasets_dir}')\n",
    "        \n",
    "    # temporary - download pseudo-manifests. please remove this in the future\n",
    "    \n",
    "    manif_list = ['https://cds.c3s.enveo.at/c3s_manifest/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_GrIS_CDR_latest.txt',\n",
    "                'https://cds.c3s.enveo.at/c3s_manifest/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_GrIS_ICDR_latest.txt',\n",
    "                'https://cds.c3s.enveo.at/c3s_manifest/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_AIS_CDR_latest.txt',\n",
    "                'https://cds.c3s.enveo.at/c3s_manifest/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_AIS_ICDR_latest.txt']\n",
    "    for manif in manif_list:\n",
    "        os.system(f'wget {manif} -P {datasets_dir}')\n",
    "\n",
    "    # download latest versions if needed\n",
    "    if region in ['Ant']:\n",
    "        dsets = ['AntIS_CDR','AntIS_ICDR','AIS_CDR','AIS_ICDR']\n",
    "    elif region in ['Gr']:\n",
    "        dsets= ['GrIS_CDR','GrIS_CDR']\n",
    "    else:\n",
    "        print('please set the region correctly for Ice Sheet SEC!!')\n",
    "        raise SystemExit\n",
    "    \n",
    "    print(dsets)\n",
    "    \n",
    "    \n",
    "    for dset in dsets:\n",
    "        if len(glob(f'{datasets_dir}/*{dset}_latest.txt')) > 0:\n",
    "            fname_manif=glob(f'{datasets_dir}/*{dset}_latest.txt')[0]\n",
    "        else:\n",
    "            continue\n",
    "        fname_manif=glob(f'{datasets_dir}/*{dset}_latest.txt')[0] # we might have to revert to this\n",
    "        # fname_manif=glob(f'{datasets_dir}/*{dset}_latest.txt')\n",
    "        \n",
    "        if not os.path.exists(fname_manif): \n",
    "            continue\n",
    "\n",
    "        with open(fname_manif) as f:\n",
    "            flist=f.readlines()\n",
    "        fname_gris=flist[-1].replace('\\n','').split('/')[-1]\n",
    "        if os.path.exists(f'{datasets_dir}{fname_gris}'):\n",
    "            print(f'{fname_gris} file exists!')\n",
    "        else:\n",
    "            print(f'Downloading {fname_gris} ...')\n",
    "            os.system(f'wget {flist[-1].replace('\\n','')} -P {datasets_dir}')\n",
    "\n",
    "    if region in ['Gr']: fnames = glob(datasets_dir+'C3S_*GrIS_RA*.nc')\n",
    "    if region in ['Ant']: fnames = glob(datasets_dir+'C3S_*AntIS_RA*.nc')+glob(datasets_dir+'C3S_*AIS_RA*.nc')\n",
    "    print(fnames)\n",
    "    datebegs = []\n",
    "    dateends = []\n",
    "    for fname in fnames:\n",
    "        print('will open '+fname)\n",
    "        nc = xr.open_dataset(fname)\n",
    "        datebegs.append(pd.Timestamp(nc['time'].values[0]))\n",
    "        dateends.append(pd.Timestamp(nc['time'].values[-1]))\n",
    "    return min(datebegs),max(dateends)\n",
    "def extract_dates_massbalance(datasets_dir):\n",
    "    # -- Ice Sheet Gravimetric Mass Balance\n",
    "    cads_forms_yml_dir=conf['cads_forms_yml_dir']\n",
    "    product='Ice Sheet Gravimetric Mass Balance' # contains all info for IS-SEC\n",
    "    entry=conf['PRODUCT'][product]['entry'][0]\n",
    "    fname_generate=f'{cads_forms_yml_dir}/{entry}/gecko-config/generate.yaml'\n",
    "    with open(fname_generate) as f:\n",
    "        generate= yaml.safe_load(f)\n",
    "    \n",
    "    #download all manifest files -- temporary disabled\n",
    "    # os.system(f'rm -rf {datasets_dir}/manifest*')\n",
    "    # for i in range(len(generate['manifest'])):\n",
    "    #     os.system(f'wget {generate['manifest'][i]} -P {datasets_dir}')\n",
    "    \n",
    "    # temporary\n",
    "    os.system(f'wget https://cds:5cXcRskEvMoMHrJQtxfy@cds.c3s.eodc.eu/manifest/manifest_c3s2_312a_eodc_ice_sheets_gravimetry_latest.txt -P {datasets_dir}')\n",
    "    # download latest versions if needed\n",
    "    print(glob(f'{datasets_dir}/*gravimetry_latest.txt'))\n",
    "    fname_manif=glob(f'{datasets_dir}/*gravimetry_latest.txt')[0]\n",
    "    \n",
    "    with open(fname_manif) as f:\n",
    "        flist=f.readlines()\n",
    "    fname_gmb=flist[-1].replace('\\n','').split('/')[-1]\n",
    "    if os.path.exists(f'{datasets_dir}{fname_gmb}'):\n",
    "        print(f'{fname_gmb} file exists!')\n",
    "    else:\n",
    "        print(f'Downloading {fname_gmb} ...')\n",
    "        os.system(f'wget {flist[-1].replace('\\n','')} -P {datasets_dir}')\n",
    "    \n",
    "    \n",
    "    fname = f'{datasets_dir}{fname_gmb}'\n",
    "    nc = xr.open_dataset(fname)\n",
    "    return pd.Timestamp(nc['time'].values[0]),pd.Timestamp(nc['time'].values[-1])\n",
    "def extract_dates_derived_glaciers(jfile):\n",
    "    with open(jfile) as f:\n",
    "        gen= yaml.safe_load(f)[0]\n",
    "    ymin = int(gen['hydrological_year'][0][0:4])\n",
    "    ymax = int(gen['hydrological_year'][-1][0:4])+1\n",
    "    mmax=9\n",
    "    dmax=30\n",
    "    mmin=4\n",
    "    dmin=1\n",
    "    print('Glaciers ',ymin,ymax)\n",
    "    return pd.Timestamp(f'{ymin}-{mmin}-{dmin}'), pd.Timestamp(f'{ymax}-{mmax}-{dmax}')\n",
    "def extract_dates_lake_levels(datasets_dir):\n",
    "    cads_forms_yml_dir=conf['cads_forms_yml_dir']\n",
    "    product='Lake Water Level' # contains all info for IS-SEC\n",
    "    entry=conf['PRODUCT'][product]['entry'][0]\n",
    "    fname_generate=f'{cads_forms_yml_dir}/{entry}/gecko-config/generate.yaml'\n",
    "    with open(fname_generate) as f:\n",
    "        generate= yaml.safe_load(f)\n",
    "    # print(generate['manifest'])\n",
    "\n",
    "    #download all manifest files\n",
    "    os.system(f'rm -rf {datasets_dir}/manifest*')\n",
    "    os.system(f'wget {generate['manifest'][-1]} -P {datasets_dir}')\n",
    "\n",
    "    fname_manif=glob(f'{datasets_dir}/manifest_*lakes_lwl_latest.txt')[0]\n",
    "    print(fname_manif)\n",
    "\n",
    "    #load manifest files and retrieve time coverage from the different filenames\n",
    "    with open(fname_manif) as f:\n",
    "        flist=f.readlines()\n",
    "    datebeg=min([pd.to_datetime(flist[i].replace('\\n','').split('/')[-1].split('_')[-3]) for i in range(len(flist))])\n",
    "    dateend=max([pd.to_datetime(flist[i].replace('\\n','').split('/')[-1].split('_')[-2]) for i in range(len(flist))])\n",
    "    return pd.Timestamp(datebeg),pd.Timestamp(dateend)\n",
    "def datemax2(row):\n",
    "    row = row.dropna()\n",
    "    ymax = int(max(row['year']))\n",
    "    if 'month' in row.keys():\n",
    "        mmax = int(max(row['month']))\n",
    "    else:\n",
    "        mmax=12\n",
    "    xx,dmax=monthrange(ymax,mmax)\n",
    "    datemax = pd.Timestamp(f'{ymax}-{mmax}-{dmax}')\n",
    "    return datemax\n",
    "def datemin2(row):\n",
    "    row = row.dropna()\n",
    "    ymin = int(min(row['year']))\n",
    "    if 'month' in row.keys():\n",
    "        mmin = int(min(row['month']))\n",
    "    else:\n",
    "        mmin=1\n",
    "    xx,dmin=monthrange(ymin,mmin)\n",
    "    datemin = pd.Timestamp(f'{ymin}-{mmin}-{dmin}')\n",
    "    return datemin\n",
    "def extract_dates_wv(jfilepath):\n",
    "    f = open(jfilepath)\n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    data = json.load(f)\n",
    "    # print(data)\n",
    "    df = pd.DataFrame(data)\n",
    "    df['datemax'] = df.apply(datemax2,axis=1)\n",
    "    df['datemin'] = df.apply(datemin2,axis=1)\n",
    "    datemin = df['datemin'].min()\n",
    "    datemax = df['datemax'].max()\n",
    "    return datemin,datemax \n",
    "def check_time_agg(row):\n",
    "    row = row.dropna()\n",
    "    if 'time_aggregation' in row.keys():\n",
    "        time_aggregation = row['time_aggregation'][0]\n",
    "        # print('time_agg',time_aggregation)\n",
    "        if time_aggregation in ['daily_average','daily_mean','day','day_average']: \n",
    "            time_agg = 'day'\n",
    "            return time_agg\n",
    "        elif time_aggregation == 'daily':\n",
    "            time_agg='daily'\n",
    "            return time_agg\n",
    "        #note interim solution for 5-daily-composite...\n",
    "        elif time_aggregation in [\n",
    "            'monthly_average',\n",
    "            '5_daily_composite',\n",
    "            'monthly_mean',\n",
    "            '27_days',\n",
    "            'month',\n",
    "            'monthly',\n",
    "            'month_average',\n",
    "            '10_day_average', # debatable..\n",
    "            ]:\n",
    "            time_agg = 'monthly'\n",
    "            return time_agg\n",
    "        else:\n",
    "            # print(row)\n",
    "            print('Could not determine time_agg')\n",
    "            raise SystemExit\n",
    "    elif 'period' in row.keys(): # applies to ice_sheets\n",
    "        time_agg = 'period'\n",
    "        return time_agg\n",
    "    elif 'temporal_aggregation' in row.keys():\n",
    "        time_aggregation = row['temporal_aggregation'][0]\n",
    "        if time_aggregation in ['monthly','6-hourly']:\n",
    "            time_agg='monthly'\n",
    "        elif time_aggregation == 'daily':\n",
    "            time_agg='daily'\n",
    "        else:\n",
    "            print('Error in temporal aggregation')\n",
    "            raise SystemExit\n",
    "        return time_agg\n",
    "    else:\n",
    "        if ('day' in row.keys()):\n",
    "            time_agg='day'\n",
    "            return time_agg\n",
    "        elif  'nominal_day' in row.keys():\n",
    "            time_agg='nominal_day'\n",
    "            return time_agg\n",
    "        else:\n",
    "            time_agg = 'monthly'\n",
    "            return time_agg\n",
    "def compute_datemax(row):\n",
    " \n",
    "    time_agg=check_time_agg(row) # check time aggregation of data in this row\n",
    "    # print('time_agg',time_agg)\n",
    "    if time_agg =='period':\n",
    "        per_str=max(row['period'])\n",
    "        ymax=int(per_str[5::])\n",
    "        mmax=9\n",
    "        dmax=30\n",
    "    else: \n",
    "        ymax = int(max(row['year']))\n",
    "        if 'month' in row.keys():\n",
    "            mmax = int(max(row['month']))\n",
    "        else:\n",
    "            mmax=12\n",
    "        xx,ndays = monthrange(ymax,mmax)\n",
    "        if time_agg in ['day','nominal_day']:\n",
    "            # print(row[time_agg])\n",
    "            dmax = int(max(row[time_agg])) \n",
    "        elif time_agg in ['daily']:\n",
    "            dmax = int(max(row['day'])) \n",
    "        else:\n",
    "            dmax = ndays # last day of month\n",
    "        if dmax >ndays:\n",
    "            print('Beware error in allowed dates...')\n",
    "            # print(row)\n",
    "            dmax=ndays\n",
    "\n",
    "    datemax = pd.Timestamp(f'{ymax}-{mmax}-{dmax}')\n",
    "    return datemax\n",
    "def compute_datemin(row):\n",
    "    time_agg=check_time_agg(row) # check time aggregation of data in this row\n",
    "    if time_agg =='period':\n",
    "        per_str=max(row['period'])\n",
    "        ymin=int(per_str[0:4])\n",
    "        mmin=10\n",
    "        dmin=1\n",
    "    else:\n",
    "        ymin = int(min(row['year'])) \n",
    "        if 'month' in row.keys():\n",
    "            mmin = int(min(row['month']))\n",
    "        else:\n",
    "            mmin=1\n",
    "        if time_agg in ['day','nominal_day']:\n",
    "            dmin = int(min(row[time_agg])) \n",
    "        elif time_agg in ['daily']:\n",
    "            dmin = int(min(row['day'])) \n",
    "        else:\n",
    "            dmin = 1 # first day of month\n",
    "        xx,ndays = monthrange(ymin,mmin)     \n",
    "        if dmin>ndays:\n",
    "            print('Beware error in allowed dates...')\n",
    "            # print(row)\n",
    "            dmin=1\n",
    "    datemin = pd.Timestamp(f'{ymin}-{mmin}-{dmin}')\n",
    "    return datemin\n",
    "def calc_dateminmax_from_cds_form(jfilepath,ecv):\n",
    "    # Opening JSON file\n",
    "    f = open(jfilepath)\n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    data = json.load(f)\n",
    "    # print(data)\n",
    "    df = pd.DataFrame(data)\n",
    "    # display(df)\n",
    "    # print(df.keys())\n",
    "    # print(len(df))\n",
    "    \n",
    "    # find records where dates cannot be defined\n",
    "    if 'sensor_and_algorithm' in df.keys():        \n",
    "        lst_erase=[]\n",
    "        for i in range(len(df)):\n",
    "            if (df['sensor_and_algorithm'][i][0]=='merged_obs4mips'): lst_erase.append(i)\n",
    "        # now .drop these problematic rows\n",
    "        for i in lst_erase:\n",
    "            df=df.drop(lst_erase)\n",
    "    if ecv == 'Earth Radiation Budget':        \n",
    "        lst_erase=[]\n",
    "        for i in range(len(df)):\n",
    "            if (df['variable'][i][0]=='total_solar_irradiance'): lst_erase.append(i) # this info is read from the dataset itself\n",
    "        # now .drop these problematic rows\n",
    "        for i in lst_erase:\n",
    "            df=df.drop(lst_erase)\n",
    "    # for i in range(len(df)):\n",
    "    #     print(df.loc[i])\n",
    "    #     if ('year' not in df[i]): lst_erase.append(i)\n",
    "\n",
    "    df['datemax'] = df.apply(compute_datemax,axis=1)\n",
    "    df['datemin'] = df.apply(compute_datemin,axis=1)\n",
    "    datemin = df['datemin'].min()\n",
    "    datemax = df['datemax'].max()\n",
    "    return datemin,datemax\n",
    "\n",
    "with open('config-athos.yml') as f:\n",
    "    conf= yaml.safe_load(f)\n",
    "\n",
    "cds_form_dir=conf['cds_form_dir']\n",
    "datasets_dir = conf['datasets_dir']\n",
    "thisyear=dt.datetime.today().year\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILD PANDAS DATAFRAME FOR TIME COVERAGE BY ECV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aerosols\n",
      "['satellite-aerosol-properties']\n",
      "/home/cxjo/C3S_stuff/cads-forms-json/satellite-aerosol-properties/constraints.json\n",
      "Albedo\n",
      "['satellite-albedo']\n",
      "/home/cxjo/C3S_stuff/cads-forms-json/satellite-albedo/constraints.json\n",
      "Greenhouse Gases\n",
      "['satellite-carbon-dioxide', 'satellite-methane']\n",
      "/home/cxjo/C3S_stuff/cads-forms-json/satellite-carbon-dioxide/constraints.json\n",
      "/home/cxjo/C3S_stuff/cads-forms-json/satellite-methane/constraints.json\n",
      "Clouds\n",
      "['satellite-cloud-properties']\n",
      "/home/cxjo/C3S_stuff/cads-forms-json/satellite-cloud-properties/constraints.json\n",
      "Earth Radiation Budget\n",
      "['satellite-earth-radiation-budget']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/etc/ecmwf/ssd/ssd1/tmpdirs/cxjo.2225094.20250612_144930.323/ipykernel_2249583/815761249.py:35: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  c=pd.read_csv(url,skiprows=128,delim_whitespace=True,header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cxjo/C3S_stuff/cads-forms-json/satellite-earth-radiation-budget/constraints.json\n",
      "Fire\n",
      "['satellite-fire-burned-area', 'satellite-fire-radiative-power']\n",
      "/home/cxjo/C3S_stuff/cads-forms-json/satellite-fire-burned-area/constraints.json\n",
      "/home/cxjo/C3S_stuff/cads-forms-json/satellite-fire-radiative-power/constraints.json\n",
      "Ice Sheets\n",
      "['satellite-greenland-ice-sheet-velocity', 'satellite-ice-sheet-elevation-change', 'satellite-ice-sheet-mass-balance']\n",
      "/home/cxjo/C3S_stuff/cads-forms-json/satellite-greenland-ice-sheet-velocity/constraints.json\n",
      "/home/cxjo/C3S_stuff/cads-forms-json/satellite-ice-sheet-elevation-change/constraints.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2025-06-12 15:26:11--  https://cds.c3s.enveo.at/c3s_manifest/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_GrIS_CDR_latest.txt\n",
      "Resolving cds.c3s.enveo.at (cds.c3s.enveo.at)... 83.175.116.100\n",
      "Connecting to cds.c3s.enveo.at (cds.c3s.enveo.at)|83.175.116.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 630 [text/plain]\n",
      "Saving to: ‘/home/cxjo/C3S_stuff/datasets/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_GrIS_CDR_latest.txt’\n",
      "\n",
      "     0K                                                       100% 38.2M=0s\n",
      "\n",
      "2025-06-12 15:26:11 (38.2 MB/s) - ‘/home/cxjo/C3S_stuff/datasets/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_GrIS_CDR_latest.txt’ saved [630/630]\n",
      "\n",
      "--2025-06-12 15:26:11--  https://cds.c3s.enveo.at/c3s_manifest/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_GrIS_ICDR_latest.txt\n",
      "Resolving cds.c3s.enveo.at (cds.c3s.enveo.at)... 83.175.116.100\n",
      "Connecting to cds.c3s.enveo.at (cds.c3s.enveo.at)|83.175.116.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 258 [text/plain]\n",
      "Saving to: ‘/home/cxjo/C3S_stuff/datasets/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_GrIS_ICDR_latest.txt’\n",
      "\n",
      "     0K                                                       100% 9.41M=0s\n",
      "\n",
      "2025-06-12 15:26:13 (9.41 MB/s) - ‘/home/cxjo/C3S_stuff/datasets/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_GrIS_ICDR_latest.txt’ saved [258/258]\n",
      "\n",
      "--2025-06-12 15:26:13--  https://cds.c3s.enveo.at/c3s_manifest/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_AIS_CDR_latest.txt\n",
      "Resolving cds.c3s.enveo.at (cds.c3s.enveo.at)... 83.175.116.100\n",
      "Connecting to cds.c3s.enveo.at (cds.c3s.enveo.at)|83.175.116.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 502 [text/plain]\n",
      "Saving to: ‘/home/cxjo/C3S_stuff/datasets/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_AIS_CDR_latest.txt’\n",
      "\n",
      "     0K                                                       100% 30.4M=0s\n",
      "\n",
      "2025-06-12 15:26:13 (30.4 MB/s) - ‘/home/cxjo/C3S_stuff/datasets/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_AIS_CDR_latest.txt’ saved [502/502]\n",
      "\n",
      "--2025-06-12 15:26:13--  https://cds.c3s.enveo.at/c3s_manifest/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_AIS_ICDR_latest.txt\n",
      "Resolving cds.c3s.enveo.at (cds.c3s.enveo.at)... 83.175.116.100\n",
      "Connecting to cds.c3s.enveo.at (cds.c3s.enveo.at)|83.175.116.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 125 [text/plain]\n",
      "Saving to: ‘/home/cxjo/C3S_stuff/datasets/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_AIS_ICDR_latest.txt’\n",
      "\n",
      "     0K                                                       100%  542M=0s\n",
      "\n",
      "2025-06-12 15:26:13 (542 MB/s) - ‘/home/cxjo/C3S_stuff/datasets/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_AIS_ICDR_latest.txt’ saved [125/125]\n",
      "\n",
      "--2025-06-12 15:26:13--  https://cds.c3s.enveo.at/service/ice_sheets/surface_elevation_change/AIS/CDR/vers5/C3S_AIS_RA_SEC_25km_vers5_2024-12-15.nc\n",
      "Resolving cds.c3s.enveo.at (cds.c3s.enveo.at)... 83.175.116.100\n",
      "Connecting to cds.c3s.enveo.at (cds.c3s.enveo.at)|83.175.116.100|:443... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AntIS_CDR', 'AntIS_ICDR', 'AIS_CDR', 'AIS_ICDR']\n",
      "Downloading C3S_AIS_RA_SEC_25km_vers5_2024-12-15.nc ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "connected.\n",
      "HTTP request sent, awaiting response... 401 Unauthorized\n",
      "\n",
      "Username/Password Authentication Failed.\n",
      "--2025-06-12 15:26:14--  https://cds.c3s.enveo.at/service/ice_sheets/surface_elevation_change/AIS/ICDR/vers5/C3S_AIS_RA_SEC_25km_vers5_2025-04-28.nc\n",
      "Resolving cds.c3s.enveo.at (cds.c3s.enveo.at)... 83.175.116.100\n",
      "Connecting to cds.c3s.enveo.at (cds.c3s.enveo.at)|83.175.116.100|:443... connected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading C3S_AIS_RA_SEC_25km_vers5_2025-04-28.nc ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP request sent, awaiting response... 401 Unauthorized\n",
      "\n",
      "Username/Password Authentication Failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/cxjo/C3S_stuff/datasets/C3S_AntIS_RA_SEC_25km_vers4_2022-12-16.nc', '/home/cxjo/C3S_stuff/datasets/C3S_AntIS_RA_SEC_vers3_2023-01-16.nc']\n",
      "will open /home/cxjo/C3S_stuff/datasets/C3S_AntIS_RA_SEC_25km_vers4_2022-12-16.nc\n",
      "will open /home/cxjo/C3S_stuff/datasets/C3S_AntIS_RA_SEC_vers3_2023-01-16.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2025-06-12 15:26:14--  https://cds.c3s.enveo.at/c3s_manifest/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_GrIS_CDR_latest.txt\n",
      "Resolving cds.c3s.enveo.at (cds.c3s.enveo.at)... 83.175.116.100\n",
      "Connecting to cds.c3s.enveo.at (cds.c3s.enveo.at)|83.175.116.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 630 [text/plain]\n",
      "Saving to: ‘/home/cxjo/C3S_stuff/datasets/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_GrIS_CDR_latest.txt’\n",
      "\n",
      "     0K                                                       100% 28.2M=0s\n",
      "\n",
      "2025-06-12 15:26:15 (28.2 MB/s) - ‘/home/cxjo/C3S_stuff/datasets/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_GrIS_CDR_latest.txt’ saved [630/630]\n",
      "\n",
      "--2025-06-12 15:26:15--  https://cds.c3s.enveo.at/c3s_manifest/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_GrIS_ICDR_latest.txt\n",
      "Resolving cds.c3s.enveo.at (cds.c3s.enveo.at)... 83.175.116.100\n",
      "Connecting to cds.c3s.enveo.at (cds.c3s.enveo.at)|83.175.116.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 258 [text/plain]\n",
      "Saving to: ‘/home/cxjo/C3S_stuff/datasets/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_GrIS_ICDR_latest.txt’\n",
      "\n",
      "     0K                                                       100% 9.76M=0s\n",
      "\n",
      "2025-06-12 15:26:16 (9.76 MB/s) - ‘/home/cxjo/C3S_stuff/datasets/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_GrIS_ICDR_latest.txt’ saved [258/258]\n",
      "\n",
      "--2025-06-12 15:26:16--  https://cds.c3s.enveo.at/c3s_manifest/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_AIS_CDR_latest.txt\n",
      "Resolving cds.c3s.enveo.at (cds.c3s.enveo.at)... 83.175.116.100\n",
      "Connecting to cds.c3s.enveo.at (cds.c3s.enveo.at)|83.175.116.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 502 [text/plain]\n",
      "Saving to: ‘/home/cxjo/C3S_stuff/datasets/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_AIS_CDR_latest.txt’\n",
      "\n",
      "     0K                                                       100% 27.2M=0s\n",
      "\n",
      "2025-06-12 15:26:17 (27.2 MB/s) - ‘/home/cxjo/C3S_stuff/datasets/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_AIS_CDR_latest.txt’ saved [502/502]\n",
      "\n",
      "--2025-06-12 15:26:17--  https://cds.c3s.enveo.at/c3s_manifest/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_AIS_ICDR_latest.txt\n",
      "Resolving cds.c3s.enveo.at (cds.c3s.enveo.at)... 83.175.116.100\n",
      "Connecting to cds.c3s.enveo.at (cds.c3s.enveo.at)|83.175.116.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 125 [text/plain]\n",
      "Saving to: ‘/home/cxjo/C3S_stuff/datasets/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_AIS_ICDR_latest.txt’\n",
      "\n",
      "     0K                                                       100%  361M=0s\n",
      "\n",
      "2025-06-12 15:26:17 (361 MB/s) - ‘/home/cxjo/C3S_stuff/datasets/pseudo-manifest_c3s2_313d_ENVEO_ice_sheets_surface_AIS_ICDR_latest.txt’ saved [125/125]\n",
      "\n",
      "--2025-06-12 15:26:17--  https://cds.c3s.enveo.at/service/ice_sheets/surface_elevation_change/GrIS/CDR/vers6/C3S_GrIS_RA_SEC_25km_Vers6_2024-12-09.nc\n",
      "Resolving cds.c3s.enveo.at (cds.c3s.enveo.at)... 83.175.116.100\n",
      "Connecting to cds.c3s.enveo.at (cds.c3s.enveo.at)|83.175.116.100|:443... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GrIS_CDR', 'GrIS_CDR']\n",
      "Downloading C3S_GrIS_RA_SEC_25km_Vers6_2024-12-09.nc ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "connected.\n",
      "HTTP request sent, awaiting response... 401 Unauthorized\n",
      "\n",
      "Username/Password Authentication Failed.\n",
      "--2025-06-12 15:26:17--  https://cds.c3s.enveo.at/service/ice_sheets/surface_elevation_change/GrIS/CDR/vers6/C3S_GrIS_RA_SEC_25km_Vers6_2024-12-09.nc\n",
      "Resolving cds.c3s.enveo.at (cds.c3s.enveo.at)... 83.175.116.100\n",
      "Connecting to cds.c3s.enveo.at (cds.c3s.enveo.at)|83.175.116.100|:443... connected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading C3S_GrIS_RA_SEC_25km_Vers6_2024-12-09.nc ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP request sent, awaiting response... 401 Unauthorized\n",
      "\n",
      "Username/Password Authentication Failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/cxjo/C3S_stuff/datasets/C3S_GrIS_RA_SEC_25km_vers4_2022-10-20.nc', '/home/cxjo/C3S_stuff/datasets/C3S_GrIS_RA_SEC_25km_vers5_2023-12-15.nc', '/home/cxjo/C3S_stuff/datasets/C3S_GrIS_RA_SEC_25km_vers5_2024-07-07.nc']\n",
      "will open /home/cxjo/C3S_stuff/datasets/C3S_GrIS_RA_SEC_25km_vers4_2022-10-20.nc\n",
      "will open /home/cxjo/C3S_stuff/datasets/C3S_GrIS_RA_SEC_25km_vers5_2023-12-15.nc\n",
      "will open /home/cxjo/C3S_stuff/datasets/C3S_GrIS_RA_SEC_25km_vers5_2024-07-07.nc\n",
      "/home/cxjo/C3S_stuff/cads-forms-json/satellite-ice-sheet-mass-balance/constraints.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2025-06-12 15:26:18--  https://cds:*password*@cds.c3s.eodc.eu/manifest/pseudo-manifest_c3s2_312a_eodc_ice_sheets_gravimetry_latest.txt\n",
      "Resolving cds.c3s.eodc.eu (cds.c3s.eodc.eu)... 193.170.203.81\n",
      "Connecting to cds.c3s.eodc.eu (cds.c3s.eodc.eu)|193.170.203.81|:443... connected.\n",
      "HTTP request sent, awaiting response... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401 Unauthorized\n",
      "Authentication selected: Basic realm=\"traefik\"\n",
      "Reusing existing connection to cds.c3s.eodc.eu:443.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2025-06-12 15:26:18 ERROR 404: Not Found.\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m     datemax_list\u001b[38;5;241m.\u001b[39mappend(datemax)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m entry \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msatellite-ice-sheet-mass-balance\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 25\u001b[0m     datemin,datemax \u001b[38;5;241m=\u001b[39m \u001b[43mextract_dates_massbalance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     datemin_list\u001b[38;5;241m.\u001b[39mappend(datemin)\n\u001b[1;32m     27\u001b[0m     datemax_list\u001b[38;5;241m.\u001b[39mappend(datemax)\n",
      "Cell \u001b[0;32mIn[16], line 126\u001b[0m, in \u001b[0;36mextract_dates_massbalance\u001b[0;34m(datasets_dir)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# download latest versions if needed\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(glob(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatasets_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/*gravimetry_latest.txt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m--> 126\u001b[0m fname_manif\u001b[38;5;241m=\u001b[39m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdatasets_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/*gravimetry_latest.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fname_manif) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    129\u001b[0m     flist\u001b[38;5;241m=\u001b[39mf\u001b[38;5;241m.\u001b[39mreadlines()\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "datesbeg = {}\n",
    "datesend = {}\n",
    "ecv_dic = {}\n",
    "for k,ecv in enumerate(conf['ECV']):\n",
    "    print(ecv)\n",
    "    entries = conf['ECV'][ecv]['entry']\n",
    "    print(entries)\n",
    "    datemin_list = []\n",
    "    datemax_list = []\n",
    "    if ecv in ['Earth Radiation Budget']: \n",
    "        datemin,datemax = extract_dates_from_TSI()\n",
    "        datemin_list.append(datemin)\n",
    "        datemax_list.append(datemax)\n",
    "    for entry in entries:\n",
    "        jfilepath=f'{cds_form_dir}{entry}/constraints.json'\n",
    "        print(jfilepath)\n",
    "        if entry == 'satellite-ice-sheet-elevation-change':\n",
    "            datemin,datemax = extract_dates_icesheets(datasets_dir,'Ant')\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "            datemin,datemax = extract_dates_icesheets(datasets_dir,'Gr')\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        elif entry == 'satellite-ice-sheet-mass-balance':\n",
    "            datemin,datemax = extract_dates_massbalance(datasets_dir)\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        elif entry == 'derived-gridded-glacier-mass-change':\n",
    "            jfile = f'{cds_form_dir}{entry}/constraints.json'\n",
    "            datemin,datemax = extract_dates_derived_glaciers(jfile)\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        elif entry == 'insitu-glaciers-extent':    \n",
    "            datemin= pd.Timestamp('1990-01-01') # http://www.glims.org/rgi_user_guide/06_dataset_summary.html\n",
    "            datemax= pd.Timestamp('2010-12-31')\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        elif entry =='satellite-total-column-water-vapour-ocean':\n",
    "            # temporal aggregation is messed up. does not have the same meaning as other datasets\n",
    "            # monthly should be yearly\n",
    "            # 6-hourly should be monthly\n",
    "            # need to write a special function that accounts for this\n",
    "            datemin,datemax = extract_dates_wv(jfilepath)\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        elif entry == 'satellite-lake-water-level':\n",
    "            datemin,datemax = extract_dates_lake_levels(datasets_dir)\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "        # elif jfilepath == '/Users/cxjo/Documents/cds-forms-c3s/satellite-land-cover/constraints.json':\n",
    "        #     datemin_list.append(pd.Timestamp('1992-01-01'))\n",
    "        #     datemax_list.append(pd.Timestamp('2022-12-31'))\n",
    "        else:\n",
    "            # print(jfilepath)\n",
    "            datemin,datemax = calc_dateminmax_from_cds_form(jfilepath,ecv)\n",
    "            # print(ecv,datemin_list,datemax_list)\n",
    "            datemin_list.append(datemin)\n",
    "            datemax_list.append(datemax)\n",
    "            # print(ecv,datemin_list,datemax_list)\n",
    "\n",
    "    # now get the max and min per ECV, accounting for all products\n",
    "    datemin_list = np.array(datemin_list)\n",
    "    datemax_list = np.array(datemax_list)\n",
    "    # datesbeg[ecv] = np.min(datemin_list)\n",
    "    # datesend[ecv] = np.max(datemax_list)\n",
    "    ecv_dic[k] = {\n",
    "        'ECV'     : ecv,\n",
    "        'DateBeg' : np.min(datemin_list),\n",
    "        'DateEnd' : np.max(datemax_list),\n",
    "        'Thematic Hub' : conf['ECV'][ecv]['Thematic_hub']\n",
    "    }\n",
    "\n",
    "\n",
    "# ecv_pd = pd.DataFrame([conf['ECV'].keys(),datesbeg,datesend],index=['DateBeg','DateEnd']).T\n",
    "ecv_pd = pd.DataFrame.from_dict(ecv_dic,orient='index').sort_values(['Thematic Hub'])\n",
    "ecv_pd['DateBeg'] = ecv_pd['DateBeg'].dt.ceil(freq='s')  \n",
    "ecv_pd['DateEnd'] = ecv_pd['DateEnd'].dt.ceil(freq='s')  \n",
    "ecv_pd['DateEnd'] = ecv_pd['DateEnd'].apply(lambda dt: dt.strftime(\"%Y-%m-%d\"))\n",
    "ecv_pd['DateBeg'] = ecv_pd['DateBeg'].apply(lambda dt: dt.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "print(ecv_pd.to_markdown())\n",
    "ecv_pd.to_excel('ECV_time_coverage_perECV.xlsx')\n",
    "# fig = px.timeline(ecv_pd, x_start=\"DateBeg\", x_end=\"DateEnd\", y='Product',color='Lot')\n",
    "fig = px.timeline(ecv_pd, x_start=\"DateBeg\", x_end=\"DateEnd\",y='ECV',color='Thematic Hub')\n",
    "\n",
    "# fig = px.timeline(datasets_df, x_start=\"startdate\", x_end=\"enddate\", y='ECV')\n",
    "fig.update_yaxes(autorange=\"reversed\")\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1200,\n",
    "    height=800,\n",
    ")\n",
    "# fig.update_layout(\n",
    "#     xaxis = dict(\n",
    "#         dtick = 'Y1',\n",
    "#         tickformat=\"%Y\",\n",
    "#     )\n",
    "# )\n",
    "\n",
    "xlab = np.arange(1970,thisyear).astype('int')\n",
    "xlabtxt = [f'{i}' for i in xlab]\n",
    "\n",
    "\n",
    "fig.update_xaxes(minor=dict(ticks=\"inside\", showgrid=True))\n",
    "fig.update_layout(\n",
    "    xaxis = dict(\n",
    "        tickmode = 'array',\n",
    "        tickvals = xlab,\n",
    "        ticktext = xlabtxt\n",
    "    )\n",
    ")\n",
    "fig.update_xaxes(tickangle=-45)\n",
    "fig.update_layout(\n",
    "    xaxis = dict(\n",
    "        tickfont = dict(\n",
    "            size=10),\n",
    "        )\n",
    "    )\n",
    "fig.update_xaxes(range = ['1970-01-01',f'{thisyear}-12-31'])\n",
    "# print(fig)\n",
    "today_date = pd.Timestamp.today().strftime('%Y%m%d') \n",
    "print(today_date)\n",
    "fig.write_image(f'temporal_coverage_by_ECV_{today_date}.pdf')\n",
    "fig.write_image(f'temporal_coverage_by_ECV_{today_date}.png')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build pandas dataframe for the figure ordered by product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dateminmax_from_cds_form_2(jfilepath,ecv):\n",
    "    # Opening JSON file\n",
    "    f = open(jfilepath)\n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    data = json.load(f)\n",
    "    # print(data)\n",
    "    df = pd.DataFrame(data)\n",
    "    # display(df)\n",
    "    \n",
    "    # print(df.keys())\n",
    "    # print(len(df))\n",
    "    \n",
    "    # find records where dates cannot be defined\n",
    "    if 'sensor_and_algorithm' in df.keys():        \n",
    "        lst_erase=[]\n",
    "        for i in range(len(df)):\n",
    "            if (df['sensor_and_algorithm'][i][0]=='merged_obs4mips'): lst_erase.append(i)\n",
    "        # now .drop these problematic rows\n",
    "        for i in lst_erase:\n",
    "            df=df.drop(lst_erase)\n",
    "    if ecv == 'Earth Radiation Budget':        \n",
    "        lst_erase=[]\n",
    "        for i in range(len(df)):\n",
    "            if (df['variable'][i][0]=='total_solar_irradiance'): lst_erase.append(i) # this info is read from the dataset itself\n",
    "        # now .drop these problematic rows\n",
    "        for i in lst_erase:\n",
    "            df=df.drop(lst_erase)\n",
    "    # for i in range(len(df)):\n",
    "    #     print(df.loc[i])\n",
    "    #     if ('year' not in df[i]): lst_erase.append(i)\n",
    "\n",
    "    df['datemax'] = df.apply(compute_datemax,axis=1)\n",
    "    df['datemin'] = df.apply(compute_datemin,axis=1)\n",
    "\n",
    "    datemin = df['datemin'].min()\n",
    "    datemax = df['datemax'].max()\n",
    "\n",
    "    return datemin,datemax\n",
    "def extract_dates_dwd_products(jfilepath,product_family):\n",
    "    \n",
    "    f = open(jfilepath)\n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    data = json.load(f)\n",
    "    # print(data)\n",
    "    df = pd.DataFrame(data)\n",
    "    df_temp=df.copy()\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i,'product_family']= df_temp['product_family'][i][0]\n",
    "    df2 = df[df['product_family'] ==product_family]\n",
    "    # display(df)\n",
    "    # display(df2)\n",
    "    df2['datemax'] = df2.apply(compute_datemax,axis=1)\n",
    "    df2['datemin'] = df2.apply(compute_datemin,axis=1)\n",
    "\n",
    "    datemin = df2['datemin'].min()\n",
    "    datemax = df2['datemax'].max()\n",
    "\n",
    "    return datemin, datemax\n",
    "\n",
    "prod_dic = {}\n",
    "datemax_list = []\n",
    "datemin_list = []\n",
    "\n",
    "for k,prod in enumerate(conf['PRODUCT']):\n",
    "    # print(prod, conf['PRODUCT'][prod]['ECV'])\n",
    "    ecv = conf['PRODUCT'][prod]['ECV']\n",
    "    entry = conf['PRODUCT'][prod]['entry'][0]\n",
    "    product = conf['PRODUCT'][prod]['Product']\n",
    "    themHub = conf['PRODUCT'][prod]['Thematic_hub']\n",
    "    jfilepath=f'{cds_form_dir}{entry}/constraints.json'\n",
    "    print(jfilepath,prod)\n",
    "    if prod == 'ERB_RMIB_TSI':\n",
    "        datemin,datemax = extract_dates_from_TSI()\n",
    "    elif prod in ['CLOUDS_CLARA-A2','CLOUDS_CLARA-A3','CLOUDS_CCI_C3S',\n",
    "                  'ERB_NASA_CERES','ERB_NOAA_HIRS','ERB_CCI_C3S','ERB_CLARA-A3',\n",
    "                  'SRB_CLARA-A2','SRB_CLARA-A3','SRB_CCI_C3S']:\n",
    "        product_family=conf['PRODUCT'][prod]['product_family']\n",
    "        datemin,datemax = extract_dates_dwd_products(jfilepath,product_family=product_family)    \n",
    "    elif prod == 'Ice Sheet Gravimetric Mass Balance':\n",
    "        fname = glob(datasets_dir+'C3S_GMB*')[0]\n",
    "        nc = xr.open_dataset(fname)\n",
    "        datemin,datemax=(pd.Timestamp(nc['time'].values[0]),pd.Timestamp(nc['time'].values[-1]))\n",
    "    elif prod == 'Ice Sheet Surface Elevation Change (Antarctica)':\n",
    "        datemin,datemax = extract_dates_icesheets(datasets_dir,'Ant')\n",
    "    elif prod == 'Ice Sheet Surface Elevation Change (Greenland)':\n",
    "        datemin,datemax = extract_dates_icesheets(datasets_dir,'Gr')\n",
    "    elif prod == 'Glaciers elevation and mass change data':\n",
    "        jfile = f'{cds_form_dir}{entry}/constraints.json'\n",
    "        print(jfile)\n",
    "        datemin,datemax = extract_dates_derived_glaciers(jfile)\n",
    "        datemin_list.append(datemin)\n",
    "        datemax_list.append(datemax)\n",
    "    elif entry == 'insitu-glaciers-extent':    \n",
    "        datemin= pd.Timestamp('1990-01-01') # http://www.glims.org/rgi_user_guide/06_dataset_summary.html\n",
    "        datemax= pd.Timestamp('2010-12-31')\n",
    "        datemin_list.append(datemin)\n",
    "        datemax_list.append(datemax)\n",
    "    elif entry =='satellite-total-column-water-vapour-ocean':\n",
    "        # temporal aggregation is messed up. does not have the same meaning as other datasets\n",
    "        # monthly should be yearly\n",
    "        # 6-hourly should be monthly\n",
    "        # need to write a special function that accounts for this\n",
    "        datemin,datemax = extract_dates_wv(jfilepath)\n",
    "        datemin_list.append(datemin)\n",
    "        datemax_list.append(datemax)\n",
    "    elif entry == 'satellite-lake-water-level':\n",
    "        datemin,datemax = extract_dates_lake_levels(datasets_dir)\n",
    "        datemin_list.append(datemin)\n",
    "        datemax_list.append(datemax)\n",
    "    else:\n",
    "        datemin,datemax = calc_dateminmax_from_cds_form_2(jfilepath,ecv)\n",
    "    print(prod,datemin,datemax)\n",
    "    datemin_list.append(datemin)\n",
    "    datemax_list.append(datemax)\n",
    "    \n",
    "    prod_dic[k] = {\n",
    "    'Product': product,\n",
    "    'ECV'     : ecv,\n",
    "    'DateBeg' : datemin,\n",
    "    'DateEnd' : datemax,\n",
    "    'Thematic Hub' : themHub\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ecv_pd = pd.DataFrame([conf['ECV'].keys(),datesbeg,datesend],index=['DateBeg','DateEnd']).T\n",
    "prod_pd = pd.DataFrame.from_dict(prod_dic,orient='index').sort_values(['Thematic Hub','ECV','Product'])\n",
    "prod_pd['DateBeg'] = prod_pd['DateBeg'].dt.ceil(freq='s')  \n",
    "prod_pd['DateEnd'] = prod_pd['DateEnd'].dt.ceil(freq='s')  \n",
    "prod_pd['DateEnd'] = prod_pd['DateEnd'].apply(lambda dt: dt.strftime(\"%Y-%m-%d\"))\n",
    "prod_pd['DateBeg'] = prod_pd['DateBeg'].apply(lambda dt: dt.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "print(prod_pd.to_markdown())\n",
    "prod_pd.to_excel('ECV_time_coverage_perProduct.xlsx')\n",
    "\n",
    "# fig = px.timeline(ecv_pd, x_start=\"DateBeg\", x_end=\"DateEnd\", y='Product',color='Lot')\n",
    "fig = px.timeline(prod_pd, x_start=\"DateBeg\", x_end=\"DateEnd\",y='Product',color='Thematic Hub')\n",
    "\n",
    "# fig = px.timeline(datasets_df, x_start=\"startdate\", x_end=\"enddate\", y='ECV')\n",
    "fig.update_yaxes(autorange=\"reversed\")\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1200,\n",
    "    height=800,\n",
    ")\n",
    "# fig.update_layout(\n",
    "#     xaxis = dict(\n",
    "#         dtick = 'Y1',\n",
    "#         tickformat=\"%Y\",\n",
    "#     )\n",
    "# )\n",
    "\n",
    "xlab = np.arange(1970,thisyear).astype('int')\n",
    "xlabtxt = [f'{i}' for i in xlab]\n",
    "\n",
    "\n",
    "fig.update_xaxes(minor=dict(ticks=\"inside\", showgrid=True))\n",
    "fig.update_layout(\n",
    "    xaxis = dict(\n",
    "        tickmode = 'array',\n",
    "        tickvals = xlab,\n",
    "        ticktext = xlabtxt\n",
    "    )\n",
    ")\n",
    "fig.update_xaxes(tickangle=-45)\n",
    "fig.update_layout(\n",
    "    xaxis = dict(\n",
    "        tickfont = dict(\n",
    "            size=10),\n",
    "        )\n",
    "    )\n",
    "fig.update_xaxes(range = ['1970-01-01',f'{thisyear}-12-31'])\n",
    "# print(fig)\n",
    "today_date = pd.Timestamp.today().strftime('%Y%m%d') \n",
    "print(today_date)\n",
    "fig.write_image(f'temporal_coverage_by_Product_{today_date}.pdf')\n",
    "fig.write_image(f'temporal_coverage_by_Product_{today_date}.png')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
